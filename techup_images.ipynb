{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4954d0-97a0-4ade-921f-1827c89846c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "0b4954d0-97a0-4ade-921f-1827c89846c5",
        "outputId": "28721201-8888-412c-c2a4-41cab055cc84",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## We Use Data to Say Something Interesting About the World\n\n- You can travel and get a whole picture, like Snyder or the guy who lived in the hills.\n\n- We mostly use **data**, or some systematic information about the world.\n\n- Data, for most of us most of the time, sounds like a **spreadsheet**:\n    - A rectangular grid with cells, a matrix.\n    - A survey.\n    - Economic growth in countries.\n    - Democracy over time.\n\n- We use it to get something interesting like:\n    - Central tendencies in all the data.\n    - Relationship between two variables or columns.\n\n- Mostly statistical commands and some visualization of results are needed.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content = \"\"\"\n",
        "## We Use Data to Say Something Interesting About the World\n",
        "\n",
        "- You can travel and get a whole picture, like Snyder or the guy who lived in the hills.\n",
        "\n",
        "- We mostly use **data**, or some systematic information about the world.\n",
        "\n",
        "- Data, for most of us most of the time, sounds like a **spreadsheet**:\n",
        "    - A rectangular grid with cells, a matrix.\n",
        "    - A survey.\n",
        "    - Economic growth in countries.\n",
        "    - Democracy over time.\n",
        "\n",
        "- We use it to get something interesting like:\n",
        "    - Central tendencies in all the data.\n",
        "    - Relationship between two variables or columns.\n",
        "\n",
        "- Mostly statistical commands and some visualization of results are needed.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2abcfce-b489-45da-b529-0029ad25550b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "a2abcfce-b489-45da-b529-0029ad25550b",
        "outputId": "82e358c4-9ab2-4195-b3a4-a6ac58ede4d3",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## This is Changing a Lot with Data Science, Big Data, etc.\n\n- We have much more information about the world:\n\n    - We can have lots of **text**.\n    - The **location of rivers** next to cities.\n    - **Audio files** from parliamentary speeches.\n    - The **human genome**.\n\n- Knowing what can be done, **beyond the spreadsheet paradigm**, is important.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_2 = \"\"\"\n",
        "## This is Changing a Lot with Data Science, Big Data, etc.\n",
        "\n",
        "- We have much more information about the world:\n",
        "\n",
        "    - We can have lots of **text**.\n",
        "    - The **location of rivers** next to cities.\n",
        "    - **Audio files** from parliamentary speeches.\n",
        "    - The **human genome**.\n",
        "\n",
        "- Knowing what can be done, **beyond the spreadsheet paradigm**, is important.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5a00f9-7de5-4625-a40a-dac0064ca04c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "8f5a00f9-7de5-4625-a40a-dac0064ca04c",
        "outputId": "b6cb9b26-8d5d-4e3b-f687-ba3fa04636d5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Tech Up and Off the Record\n\n- We will show some of the ways of dealing with different data.\n\n- We:\n    - Will create a community of users.\n    - Will have people contributing workshops.\n\n- The model is:\n    - **Exposure**\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_3 = \"\"\"\n",
        "## Tech Up and Off the Record\n",
        "\n",
        "- We will show some of the ways of dealing with different data.\n",
        "\n",
        "- We:\n",
        "    - Will create a community of users.\n",
        "    - Will have people contributing workshops.\n",
        "\n",
        "- The model is:\n",
        "    - **Exposure**\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929e2609-da9d-445f-8c0b-987104568e6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "929e2609-da9d-445f-8c0b-987104568e6e",
        "outputId": "8b11187c-1ffe-4e35-da89-da7002b1c225",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## I Will Begin by Talking About STATA, R, and Python\n\n- **STATA:** Spreadsheet-like interface, command log, statistical focus, proprietary software ($$).\n- **Python (Py):** A general-purpose programming language, free and open-source, with extensive libraries for statistics and data science.\n- **R:** Positioned between a programming language and dedicated statistical software, free and open-source.\n\n- Python and R are free and rely on user-added capabilities through packages and libraries.\n\n- All three have graphical interfaces (though some are more extensive) and can be run on your local computer\n\n- In terms of jobs in the wider world (on a scale of 1 to 100):\n    - **Python:** 90\n    - **R:** 30\n    - **STATA:** 5\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_4 = \"\"\"\n",
        "## I Will Begin by Talking About STATA, R, and Python\n",
        "\n",
        "- **STATA:** Spreadsheet-like interface, command log, statistical focus, proprietary software ($$).\n",
        "- **Python (Py):** A general-purpose programming language, free and open-source, with extensive libraries for statistics and data science.\n",
        "- **R:** Positioned between a programming language and dedicated statistical software, free and open-source.\n",
        "\n",
        "- Python and R are free and rely on user-added capabilities through packages and libraries.\n",
        "\n",
        "- All three have graphical interfaces (though some are more extensive) and can be run on your local computer\n",
        "\n",
        "- In terms of jobs in the wider world (on a scale of 1 to 100):\n",
        "    - **Python:** 90\n",
        "    - **R:** 30\n",
        "    - **STATA:** 5\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02bf725-ff19-4b2c-9844-212aa12282eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "c02bf725-ff19-4b2c-9844-212aa12282eb",
        "outputId": "54773099-2cd0-4962-8d82-1fa1c33f86b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## What We Do Using a Specific Language?\n\n- We take something in the world, look at it, and **transform it into something else** using our chosen tool.\n\n- What we take can be **big or small**, and you may encounter **performance issues** if you try to store Ð¸Ñ‚ in your computer's memory.\n\n- **Big-ness** (the sheer volume of data) is only part of the issue.\n\n- **What *can* be loaded and processed efficiently** by each tool is a crucial consideration.\n\n- For example, Leo Tolstoy's *War and Peace* contains approximately **587,287 words**.\n\n- Let's load the text of *War and Peace* in STATA, R, and Python.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_5 = \"\"\"\n",
        "## What We Do Using a Specific Language?\n",
        "\n",
        "- We take something in the world, look at it, and **transform it into something else** using our chosen tool.\n",
        "\n",
        "- What we take can be **big or small**, and you may encounter **performance issues** if you try to store Ð¸Ñ‚ in your computer's memory.\n",
        "\n",
        "- **Big-ness** (the sheer volume of data) is only part of the issue.\n",
        "\n",
        "- **What *can* be loaded and processed efficiently** by each tool is a crucial consideration.\n",
        "\n",
        "- For example, Leo Tolstoy's *War and Peace* contains approximately **587,287 words**.\n",
        "\n",
        "- Let's load the text of *War and Peace* in STATA, R, and Python.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c921b313-7876-4362-a491-596288560ea9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "c921b313-7876-4362-a491-596288560ea9",
        "outputId": "565d59c6-b33e-482b-b1bc-02d3e4eb3897",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Jupyter Notebooks\n\n- Before diving into specific languages, let's talk about **Jupyter Notebooks**.\n\n- Wouldn't it be nice to be able to run **all kinds of code** (e.g., Python, R, even potentially STATA through kernels) in a single interactive environment?\n\n    - So you get a **log of what happened**.\n    - The ability to add **annotations** and explanations alongside your code.\n    - And the **output** of your code (results, visualizations) displayed directly below.\n\n- Jupyter Notebooks allow you to do exactly this, running from an HTML page, either on your local computer or on a remote server.\n\n    - In many cases, **the notebook itself can become your paper or report**, integrating code, results, and narrative.\n\n- We will also touch upon **generative AI**.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_6 = \"\"\"\n",
        "## Jupyter Notebooks\n",
        "\n",
        "- Before diving into specific languages, let's talk about **Jupyter Notebooks**.\n",
        "\n",
        "- Wouldn't it be nice to be able to run **all kinds of code** (e.g., Python, R, even potentially STATA through kernels) in a single interactive environment?\n",
        "\n",
        "    - So you get a **log of what happened**.\n",
        "    - The ability to add **annotations** and explanations alongside your code.\n",
        "    - And the **output** of your code (results, visualizations) displayed directly below.\n",
        "\n",
        "- Jupyter Notebooks allow you to do exactly this, running from an HTML page, either on your local computer or on a remote server.\n",
        "\n",
        "    - In many cases, **the notebook itself can become your paper or report**, integrating code, results, and narrative.\n",
        "\n",
        "- We will also touch upon **generative AI**.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7791bf-08b6-40d0-a969-1521ab7b8de1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "8f7791bf-08b6-40d0-a969-1521ab7b8de1",
        "outputId": "86e77f09-3425-4812-8da5-b382e2a4903e",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Counting Words in War and Peace with Python\n\n- Let's open a Python Jupyter Notebook.\n\n- Our goal is to count:\n    - The total number of words in Leo Tolstoy's *War and Peace*.\n    - How many times the main character(s) are mentioned.\n\n- To help us with this task, **let's leverage the capabilities of a generative AI model like ChatGPT.** We can ask it for Python code to:\n    - Read the text of *War and Peace* from a file.\n    - Split the text into individual words.\n    - Count the total number of words.\n    - Identify and count mentions of the main character(s) (we'll need to decide who those are!).\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_7 = \"\"\"\n",
        "## Counting Words in War and Peace with Python\n",
        "\n",
        "- Let's open a Python Jupyter Notebook.\n",
        "\n",
        "- Our goal is to count:\n",
        "    - The total number of words in Leo Tolstoy's *War and Peace*.\n",
        "    - How many times the main character(s) are mentioned.\n",
        "\n",
        "- To help us with this task, **let's leverage the capabilities of a generative AI model like ChatGPT.** We can ask it for Python code to:\n",
        "    - Read the text of *War and Peace* from a file.\n",
        "    - Split the text into individual words.\n",
        "    - Count the total number of words.\n",
        "    - Identify and count mentions of the main character(s) (we'll need to decide who those are!).\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61dcb6c-108b-412c-bd96-c82b4e0d6fc3",
      "metadata": {
        "id": "c61dcb6c-108b-412c-bd96-c82b4e0d6fc3"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_8 = \"\"\"\n",
        "## Let's change working directory\n",
        "\n",
        "- We will change to Downloads\n",
        "\n",
        "- And make sure that the file warandpeace.txt is there\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_8))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive')\n",
        "\n",
        "\n",
        "folder_path = '/content/drive/MyDrive'\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "print(\"Files in folder:\")\n",
        "for f in files:\n",
        "    print(f)\n",
        "\n",
        "print(\"Current Working Directory \" , os.getcwd())\n",
        "listOfFiles = os.listdir('.')\n",
        "#print(listOfFiles)\n",
        "#print(listOfFiles.count(\"warandpeace.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2bd0ff-30c2-42ee-8bda-526aa6dd0133",
      "metadata": {
        "id": "7d2bd0ff-30c2-42ee-8bda-526aa6dd0133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "68a84098-4cad-4a75-e0e6-a34f50e7e755",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Asking ChatGPT for Help: Counting Words\n\n- So, I went to ChatGPT and asked the following prompt:\n\n> \"I have a text file called `warandpeace.txt` in my working directory. Give me Python code which counts and displays the total number of words in the file.\"\n\n- And here's the kind of Python code ChatGPT might provide:\n\n```python\ntry:\n    with open('warandpeace.txt', 'r', encoding='utf-8') as file:\n        text = file.read()\n        words = text.split()\n        total_word_count = len(words)\n        print(f\"The total number of words in warandpeace.txt is: {total_word_count}\")\nexcept FileNotFoundError:\n    print(\"Error: The file 'warandpeace.txt' was not found in the working directory.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n-This demonstrates how we can quickly get a starting point for our data analysis tasks using generative AI.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_9 = \"\"\"\n",
        "## Asking ChatGPT for Help: Counting Words\n",
        "\n",
        "- So, I went to ChatGPT and asked the following prompt:\n",
        "\n",
        "> \"I have a text file called `warandpeace.txt` in my working directory. Give me Python code which counts and displays the total number of words in the file.\"\n",
        "\n",
        "- And here's the kind of Python code ChatGPT might provide:\n",
        "\n",
        "```python\n",
        "try:\n",
        "    with open('warandpeace.txt', 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        words = text.split()\n",
        "        total_word_count = len(words)\n",
        "        print(f\"The total number of words in warandpeace.txt is: {total_word_count}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'warandpeace.txt' was not found in the working directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "-This demonstrates how we can quickly get a starting point for our data analysis tasks using generative AI.\n",
        "\"\"\"\n",
        "display(Markdown(slide_content_9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e87cde3-4742-40a0-a49b-8f4afb95ea53",
      "metadata": {
        "id": "4e87cde3-4742-40a0-a49b-8f4afb95ea53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e67032-e8c7-45a2-d41c-499f3faf33eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words: 562488\n"
          ]
        }
      ],
      "source": [
        "# Open the file and read its content\n",
        "with open(\"warandpeace.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Split the text into words using whitespace\n",
        "words = text.split()\n",
        "\n",
        "# Count the number of words\n",
        "word_count = len(words)\n",
        "\n",
        "# Display the result\n",
        "print(f\"Total number of words: {word_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "088023bc-0eb1-4e98-822e-320fcad78623",
      "metadata": {
        "id": "088023bc-0eb1-4e98-822e-320fcad78623",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8829b843-665d-4144-c03a-1ef5f76a4f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bezukhov appears 24 times.\n",
            "Rostova appears 11 times.\n"
          ]
        }
      ],
      "source": [
        "#I have a list in python called words. Tell me how many times the names Bezukhov and Rostova appear as elements of that list.\n",
        "\n",
        "bezukhov_count = words.count(\"Bezukhov\")\n",
        "rostova_count = words.count(\"Rostova\")\n",
        "\n",
        "print(f\"Bezukhov appears {bezukhov_count} times.\")\n",
        "print(f\"Rostova appears {rostova_count} times.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3b2e18-87dc-4ac9-bc7e-894cb5f80098",
      "metadata": {
        "id": "4e3b2e18-87dc-4ac9-bc7e-894cb5f80098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "cellView": "form",
        "outputId": "a37a3647-f5a0-4e0b-d346-7ccd1fc0ca8b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Let's Ask ChatGPT for STATA and R Examples\n\n- We can also ask ChatGPT how to perform similar analyses in STATA and R:\n\n    - **STATA:** \"Give me STATA code to count the number of times 'Bezukhov' and 'Rostova' appear in a text file called warandpeace.txt.\"\n\n    - **R:** \"Give me R code to count the number of times 'Bezukhov' and 'Rostova' appear in a text file called warandpeace.txt.\"\n\n- Furthermore, we can explore more complex tasks:\n\n    - **STATA & V-Dem:** \"Give me STATA code to load the V-Dem dataset and display the number of countries classified as democracies each year over time.\"\n\n    - **R & Party Data:** \"Give me R code using ggplot2 to create a density plot of the ideological positions of political parties in Eastern Europe (assuming I have a dataset with party names and their ideological scores).\"\n\n- By prompting ChatGPT with specific questions related to these software packages and our analytical goals, we can get guidance on syntax, commands, and even suggestions for relevant packages or approaches.\n\n- This highlights the potential of generative AI to assist us across different data analysis environments.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_10 = \"\"\"\n",
        "## Let's Ask ChatGPT for STATA and R Examples\n",
        "\n",
        "- We can also ask ChatGPT how to perform similar analyses in STATA and R:\n",
        "\n",
        "    - **STATA:** \"Give me STATA code to count the number of times 'Bezukhov' and 'Rostova' appear in a text file called warandpeace.txt.\"\n",
        "\n",
        "    - **R:** \"Give me R code to count the number of times 'Bezukhov' and 'Rostova' appear in a text file called warandpeace.txt.\"\n",
        "\n",
        "- Furthermore, we can explore more complex tasks:\n",
        "\n",
        "    - **STATA & V-Dem:** \"Give me STATA code to load the V-Dem dataset and display the number of countries classified as democracies each year over time.\"\n",
        "\n",
        "    - **R & Party Data:** \"Give me R code using ggplot2 to create a density plot of the ideological positions of political parties in Eastern Europe (assuming I have a dataset with party names and their ideological scores).\"\n",
        "\n",
        "- By prompting ChatGPT with specific questions related to these software packages and our analytical goals, we can get guidance on syntax, commands, and even suggestions for relevant packages or approaches.\n",
        "\n",
        "- This highlights the potential of generative AI to assist us across different data analysis environments.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1895c4ce-aa82-464e-a382-661d494f4582",
      "metadata": {
        "id": "1895c4ce-aa82-464e-a382-661d494f4582"
      },
      "source": [
        "## files\n",
        "\n",
        "- **R_Jypter.ipynb**  \n",
        "    and\n",
        "- **TechUp.ipynb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603008d7-1e84-4019-8ef2-678689681ddf",
      "metadata": {
        "id": "603008d7-1e84-4019-8ef2-678689681ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "cellView": "form",
        "outputId": "060c0aac-6695-4ce3-ec4b-d188bc2fa960"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## I asked Chat GPT this\n\n- I am running STATA in Ju notebook. I have a dataset called V-Dem-CY-Core-v15_dta in /Users/supermarinovs/Downloads/ It is a time-series panel data of countries observed annually with variable country_id identifying the countries and variable year identifying the year. I want to create an over-time graph that gives me the mean of variable v2x_polyarchy for all countries. The x-axis should be year and y-axis the mean v2x_polyarchy for all units called country_id for that year. The x-axis should run from the year 1950 to the year 2000.\n\n- I got this (in STATA)\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_12 = \"\"\"\n",
        "## I asked Chat GPT this\n",
        "\n",
        "- I am running STATA in Ju notebook. I have a dataset called V-Dem-CY-Core-v15_dta in /Users/supermarinovs/Downloads/ It is a time-series panel data of countries observed annually with variable country_id identifying the countries and variable year identifying the year. I want to create an over-time graph that gives me the mean of variable v2x_polyarchy for all countries. The x-axis should be year and y-axis the mean v2x_polyarchy for all units called country_id for that year. The x-axis should run from the year 1950 to the year 2000.\n",
        "\n",
        "- I got this (in STATA)\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_12))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48639c4-18f6-45c7-acea-509a326fa7c1",
      "metadata": {
        "id": "a48639c4-18f6-45c7-acea-509a326fa7c1"
      },
      "source": [
        "<!-- Slide -->\n",
        "## ðŸ“Š Stata Code to Generate Polyarchy Plot\n",
        "\n",
        "```stata\n",
        "use \"/Users/supermarinovs/Downloads/V-Dem-CY-Core-v15_dta/V-Dem-CY-Core-v15.dta\", clear\n",
        "\n",
        "* Create average v2x_polyarchy by year\n",
        "collapse (mean) v2x_polyarchy, by(year)\n",
        "\n",
        "* Keep only years 1950â€“2000\n",
        "keep if year >= 1950 & year <= 2000\n",
        "\n",
        "* Plot the result\n",
        "twoway (line v2x_polyarchy year), ///\n",
        "    title(\"Mean Polyarchy Index Over Time (1950â€“2000)\") ///\n",
        "    ytitle(\"Mean v2x_polyarchy\") ///\n",
        "    xtitle(\"Year\") ///\n",
        "    xlabel(1950(10)2000) ///\n",
        "    graphregion(color(white)) ///\n",
        "    legend(off)\n",
        "\n",
        "graph export \"v2x_polyarchy_plot.png\", replace width(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfc4489-f2b2-4715-812f-b9a426bdeaea",
      "metadata": {
        "id": "adfc4489-f2b2-4715-812f-b9a426bdeaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "cellView": "form",
        "outputId": "16c220c2-2713-4fce-a19b-9dea50527b0d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Let's Recap\n\n- **Python is very flexible:**\n    - Packages like **Pandas** (data manipulation), **NumPy** (numerical computing), **SpaCy** and **NLTK** (natural language processing) make statistics, web-scraping, and language analysis easier.\n\n- **R is very versatile:**\n    - Probably the **best all-around tool for creating statistical graphs and visualizations**.\n    - Also functions as a programming language, allowing it to perform many of the same tasks as Python.\n\n- **STATA excels at handling structured, rectangular datasets (like spreadsheets):**\n    - It has **dedicated statistical support** and built-in commands for common econometric and statistical analyses.\n\n- **General Trends:**\n    - **Economics:** Often leans towards R and STATA.\n    - **Data Science and Digital Humanities:** Frequently utilize Python.\n    - **Political Science:** Has a strong tradition of using STATA.\n\n- **But really, best to use all these tools when appropriate and find ways to pass objects and data between them** to leverage strengths.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_recap = \"\"\"\n",
        "## Let's Recap\n",
        "\n",
        "- **Python is very flexible:**\n",
        "    - Packages like **Pandas** (data manipulation), **NumPy** (numerical computing), **SpaCy** and **NLTK** (natural language processing) make statistics, web-scraping, and language analysis easier.\n",
        "\n",
        "- **R is very versatile:**\n",
        "    - Probably the **best all-around tool for creating statistical graphs and visualizations**.\n",
        "    - Also functions as a programming language, allowing it to perform many of the same tasks as Python.\n",
        "\n",
        "- **STATA excels at handling structured, rectangular datasets (like spreadsheets):**\n",
        "    - It has **dedicated statistical support** and built-in commands for common econometric and statistical analyses.\n",
        "\n",
        "- **General Trends:**\n",
        "    - **Economics:** Often leans towards R and STATA.\n",
        "    - **Data Science and Digital Humanities:** Frequently utilize Python.\n",
        "    - **Political Science:** Has a strong tradition of using STATA.\n",
        "\n",
        "- **But really, best to use all these tools when appropriate and find ways to pass objects and data between them** to leverage strengths.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_recap))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e54580-49c2-41c1-88ae-24000f2b9dc3",
      "metadata": {
        "id": "e9e54580-49c2-41c1-88ae-24000f2b9dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "cellView": "form",
        "outputId": "0bcd076c-2556-4187-95f0-f029afe439a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Other Random Things Worth Knowing\n\n- **GitHub:** A platform for version control and collaboration on code and other files. Essential for managing projects and sharing your work.\n\n- **Google Colab:** Share your notebook so that it turns into a google doc - others can access it and run it in real time\n\n- **LaTeX:** A powerful typesetting system widely used for creating professional-looking documents, especially those with mathematical formulas, scientific notation, and consistent formatting.\n\n- **Surveys:** A fundamental method for collecting data about opinions, behaviors, and characteristics of a population. Understanding survey design and analysis is crucial.\n\n- **Presentations:** Effective communication of your findings is key. Learning to create engaging and informative presentations (like this one!) is a valuable skill.\n\n- **GIS (Geographic Information Systems):** Tools and technologies for analyzing and visualizing spatial data, such as maps, locations, and geographic features.\n\n- **Canvas (or other Learning Management Systems):** Platforms often used for educational purposes, sharing materials, and facilitating online learning and collaboration.\n\n- **= Simple text!** Don't underestimate the power of plain text files for storing and exchanging data and information in a simple and universal format.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_other = \"\"\"\n",
        "## Other Random Things Worth Knowing\n",
        "\n",
        "- **GitHub:** A platform for version control and collaboration on code and other files. Essential for managing projects and sharing your work.\n",
        "\n",
        "- **Google Colab:** Share your notebook so that it turns into a google doc - others can access it and run it in real time\n",
        "\n",
        "- **LaTeX:** A powerful typesetting system widely used for creating professional-looking documents, especially those with mathematical formulas, scientific notation, and consistent formatting.\n",
        "\n",
        "- **Surveys:** A fundamental method for collecting data about opinions, behaviors, and characteristics of a population. Understanding survey design and analysis is crucial.\n",
        "\n",
        "- **Presentations:** Effective communication of your findings is key. Learning to create engaging and informative presentations (like this one!) is a valuable skill.\n",
        "\n",
        "- **GIS (Geographic Information Systems):** Tools and technologies for analyzing and visualizing spatial data, such as maps, locations, and geographic features.\n",
        "\n",
        "- **Canvas (or other Learning Management Systems):** Platforms often used for educational purposes, sharing materials, and facilitating online learning and collaboration.\n",
        "\n",
        "- **= Simple text!** Don't underestimate the power of plain text files for storing and exchanging data and information in a simple and universal format.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_other))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f89364-b24c-4b9d-85b7-74505bfd48d9",
      "metadata": {
        "id": "36f89364-b24c-4b9d-85b7-74505bfd48d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "cfa6cbf7-42f3-425f-c7d2-2f34416285eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fba1b070fe0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://gwosc.org/s/events/GW150914/GW150914_tutorial.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame(\"https://gwosc.org/s/events/GW150914/GW150914_tutorial.html\", 900,500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d432c37-8c2c-41d6-87cb-bf4e495feb69",
      "metadata": {
        "id": "8d432c37-8c2c-41d6-87cb-bf4e495feb69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "0dda9eb5-853f-4877-d959-f4f16d7f7178"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fba1b0700e0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://ncea.maps.arcgis.com/apps/instant/sidebar/index.html?appid=cf571f455b444e588aa94bbd22021cd3&fbclid=IwY2xjawJfOzhleHRuA2FlbQIxMAABHr5keh3z2uIx1zXM-mN0oen0-H09cmJnXErurPtJLQcRgU-4G8g10cuW8tC5_aem_2DAhXkF_tMmVhI6oO8hhnQ\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame(\"https://ncea.maps.arcgis.com/apps/instant/sidebar/index.html?appid=cf571f455b444e588aa94bbd22021cd3&fbclid=IwY2xjawJfOzhleHRuA2FlbQIxMAABHr5keh3z2uIx1zXM-mN0oen0-H09cmJnXErurPtJLQcRgU-4G8g10cuW8tC5_aem_2DAhXkF_tMmVhI6oO8hhnQ\", 900,500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d9f39d-9955-4eb9-91b4-5228976d40e0",
      "metadata": {
        "id": "b0d9f39d-9955-4eb9-91b4-5228976d40e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "cellView": "form",
        "outputId": "ccfa4f35-6f2c-4e9d-af03-bf50f4f03c8f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## The Plan\n\n- Intro to Py and Text-Processing\n\n- Intro to Basic Graphs in R\n\n- Intro to displaying geo-coded info\n\n- Intro to creating your website\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_off_record = \"\"\"\n",
        "## The Plan\n",
        "\n",
        "- Intro to Py and Text-Processing\n",
        "\n",
        "- Intro to Basic Graphs in R\n",
        "\n",
        "- Intro to displaying geo-coded info\n",
        "\n",
        "- Intro to creating your website\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_off_record))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6043c716-82bd-4bd0-ba97-eff9ddf4c0b8",
      "metadata": {
        "id": "6043c716-82bd-4bd0-ba97-eff9ddf4c0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "cellView": "form",
        "outputId": "013d381a-19bd-4948-bd53-64ac6b9b8098"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Image Classification and Detection\n\n- You can **train a model to recognize one class of images** and distinguish it from another â€” this is often very useful.\n- Alternatively, you can **use a pre-trained model** (for example, to identify whether images contain faces, or protests).\n- The model can tell whether one class of images differs from another â€” though it **wonâ€™t tell you why**.\n- This involves **labelling** and **classification** â€” assigning meaning to patterns.\n- Some models can **detect boundaries or shapes** â€” that is, identify the company that pixels keep.\n- **Convolutional Neural Networks (CNNs)** are one common deep-learning model for such tasks.\n- Possible application: detecting **fraudulent ballots**.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "slide_image1 = \"\"\"\n",
        "## Image Classification and Detection\n",
        "\n",
        "- You can **train a model to recognize one class of images** and distinguish it from another â€” this is often very useful.\n",
        "- Alternatively, you can **use a pre-trained model** (for example, to identify whether images contain faces, or protests).\n",
        "- The model can tell whether one class of images differs from another â€” though it **wonâ€™t tell you why**.\n",
        "- This involves **labelling** and **classification** â€” assigning meaning to patterns.\n",
        "- Some models can **detect boundaries or shapes** â€” that is, identify the company that pixels keep.\n",
        "- **Convolutional Neural Networks (CNNs)** are one common deep-learning model for such tasks.\n",
        "- Possible application: detecting **fraudulent ballots**.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_image1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "slide_image2 = \"\"\"\n",
        "## Building a Research Pipeline\n",
        "\n",
        "- **Is there a theory?** Always start by grounding your classification idea conceptually.\n",
        "- **How do you get the files?** Think about reproducibility and access.\n",
        "- **Reproduction:** Make your data and code **available** to others.\n",
        "- Note on **data standards:** We lack a strong Python community in this space, so shared norms are still emerging.\n",
        "- **Co-authorship** is highly beneficial for developing and maintaining shared standards.\n",
        "- Use **GitHub** and **version control** to track collaboration and changes.\n",
        "- Remember: you can always **create categories** to organize your data and results.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_image2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "cellView": "form",
        "id": "ZzrS51RElebF",
        "outputId": "07bf03d3-4fb6-40a0-fb2e-14b0ccff9bc1"
      },
      "id": "ZzrS51RElebF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Building a Research Pipeline\n\n- **Is there a theory?** Always start by grounding your classification idea conceptually.\n- **How do you get the files?** Think about reproducibility and access.\n- **Reproduction:** Make your data and code **available** to others.\n- Note on **data standards:** We lack a strong Python community in this space, so shared norms are still emerging.\n- **Co-authorship** is highly beneficial for developing and maintaining shared standards.\n- Use **GitHub** and **version control** to track collaboration and changes.\n- Remember: you can always **create categories** to organize your data and results.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "slide_image3 = \"\"\"\n",
        "## From Project to Thesis\n",
        "\n",
        "- A project like this can easily become a **credible MA or even BA thesis**.\n",
        "- We do everything in **Python**, while showcasing opportunities to:\n",
        "    - Work directly with the **file system**,\n",
        "    - **Extract and save** image-based information.\n",
        "- Example dataset:\n",
        "    - [EP MPs Banner Images â€” Full Set](https://gunet-my.sharepoint.com/:f:/g/personal/panagiotis_nikolakopoulos_gu_se/EnhZA0yCpcpFnv7bmljQsZwB8owO4Z0QgNN4fAH-KEyxVQ?e=SbVMA7)\n",
        "    - EP_MPs_BannerImage_Log_FullSet.xlsx says more about the images\n",
        "- Suggested workflow:\n",
        "    - Place images in Google Drive folder: `ep_member_banner_img`\n",
        "    - Resize as appropriate\n",
        "    - Label faces (or not)\n",
        "    - Extract features and **merge** results\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_image3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "tKxHn0WTlfH4",
        "outputId": "2ce388db-298b-4353-9064-75ba240cc9d0"
      },
      "id": "tKxHn0WTlfH4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## From Project to Thesis\n\n- A project like this can easily become a **credible MA or even BA thesis**.\n- We do everything in **Python**, while showcasing opportunities to:\n    - Work directly with the **file system**,\n    - **Extract and save** image-based information.\n- Example dataset:\n    - [EP MPs Banner Images â€” Full Set](https://gunet-my.sharepoint.com/:f:/g/personal/panagiotis_nikolakopoulos_gu_se/EnhZA0yCpcpFnv7bmljQsZwB8owO4Z0QgNN4fAH-KEyxVQ?e=SbVMA7)\n    - EP_MPs_BannerImage_Log_FullSet.xlsx says more about the images\n- Suggested workflow:\n    - Place images in Google Drive folder: `ep_member_banner_img`\n    - Resize as appropriate\n    - Label faces (or not)\n    - Extract features and **merge** results\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "slide_image4 = \"\"\"\n",
        "## Evaluating Classification Models\n",
        "\n",
        "- In **machine learning classification**, we often measure performance using:\n",
        "    - **Precision:** How accurately the model labels positive cases\n",
        "      `Precision = True Positives / (True Positives + False Positives)`\n",
        "    - **Recall:** How well the model finds all relevant positive cases\n",
        "      `Recall = True Positives / (True Positives + False Negatives)`\n",
        "- Both are essential for assessing how well a model performs.\n",
        "- **High precision** â†’ few false positives.\n",
        "  **High recall** â†’ few false negatives.\n",
        "- Especially important for **imbalanced datasets**, where some classes are rare.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_image4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "cellView": "form",
        "id": "6VwD69s-lfT4",
        "outputId": "81ec78c3-c680-4269-c025-cbefc90aaa70"
      },
      "id": "6VwD69s-lfT4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Evaluating Classification Models\n\n- In **machine learning classification**, we often measure performance using:\n    - **Precision:** How accurately the model labels positive cases  \n      `Precision = True Positives / (True Positives + False Positives)`\n    - **Recall:** How well the model finds all relevant positive cases  \n      `Recall = True Positives / (True Positives + False Negatives)`\n- Both are essential for assessing how well a model performs.\n- **High precision** â†’ few false positives.  \n  **High recall** â†’ few false negatives.\n- Especially important for **imbalanced datasets**, where some classes are rare.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "slide_image5 = \"\"\"\n",
        "## A Beginner-Friendly Primer\n",
        "\n",
        "- For an accessible introduction to machine vision and image analysis, see:\n",
        "  [Seeing Like a Machine: A Beginnerâ€™s Guide to Image Analysis in Machine Learning](https://www.datacamp.com/tutorial/seeing-like-a-machine-a-beginners-guide-to-image-analysis-in-machine-learning)\n",
        "- A short, practical read that complements todayâ€™s discussion nicely.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_image5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "cellView": "form",
        "id": "qnQ0oLPSlfg4",
        "outputId": "1d620635-e84e-4962-d429-d72faf013617"
      },
      "id": "qnQ0oLPSlfg4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## A Beginner-Friendly Primer\n\n- For an accessible introduction to machine vision and image analysis, see:  \n  [Seeing Like a Machine: A Beginnerâ€™s Guide to Image Analysis in Machine Learning](https://www.datacamp.com/tutorial/seeing-like-a-machine-a-beginners-guide-to-image-analysis-in-machine-learning)\n- A short, practical read that complements todayâ€™s discussion nicely.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_13 = \"\"\"\n",
        "\n",
        "| Aspect                 | Text Classification                                                                             | Image Classification                                                                      |\n",
        "| ---------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
        "| **Input**              | Natural language (strings)                                                                      | Pixel arrays (matrices/tensors)                                                           |\n",
        "| **Preprocessing**      | Tokenization, stopword removal, lemmatization, embedding (Bag of Words, TFâ€“IDF, word2vec, BERT) | Normalization, resizing, data augmentation (rotation, flip, crop)                         |\n",
        "| **Feature Extraction** | Manual (TFâ€“IDF), or automatic via embeddings/transformers                                       | Often automatic (CNNs learn features), or manual descriptors (SIFT, HOG) in older methods |\n",
        "| **Classical ML**       | Naive Bayes, Logistic Regression, SVM on text features                                          | SVM, Random Forest, kNN on flattened pixel/handcrafted features                           |\n",
        "| **Deep Learning**      | RNNs, CNNs for text, Transformers (BERT, GPT-like)                                              | Convolutional Neural Networks (ResNet, VGG, EfficientNet)                                 |\n",
        "| **Libraries**          | `scikit-learn`, `nltk`, `spaCy`, `transformers`                                                 | `tensorflow.keras`, `torchvision`, `scikit-learn` (basic)                                 |\n",
        "| **Challenges**         | Handling vocabulary, context, long sequences                                                    | High dimensionality, need lots of data, overfitting                                       |\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_13))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YNKC4BI42ph9",
        "outputId": "ab9915ff-2444-4199-a49f-d89bc8836f27",
        "cellView": "form"
      },
      "id": "YNKC4BI42ph9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n| Aspect                 | Text Classification                                                                             | Image Classification                                                                      |\n| ---------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n| **Input**              | Natural language (strings)                                                                      | Pixel arrays (matrices/tensors)                                                           |\n| **Preprocessing**      | Tokenization, stopword removal, lemmatization, embedding (Bag of Words, TFâ€“IDF, word2vec, BERT) | Normalization, resizing, data augmentation (rotation, flip, crop)                         |\n| **Feature Extraction** | Manual (TFâ€“IDF), or automatic via embeddings/transformers                                       | Often automatic (CNNs learn features), or manual descriptors (SIFT, HOG) in older methods |\n| **Classical ML**       | Naive Bayes, Logistic Regression, SVM on text features                                          | SVM, Random Forest, kNN on flattened pixel/handcrafted features                           |\n| **Deep Learning**      | RNNs, CNNs for text, Transformers (BERT, GPT-like)                                              | Convolutional Neural Networks (ResNet, VGG, EfficientNet)                                 |\n| **Libraries**          | `scikit-learn`, `nltk`, `spaCy`, `transformers`                                                 | `tensorflow.keras`, `torchvision`, `scikit-learn` (basic)                                 |\n| **Challenges**         | Handling vocabulary, context, long sequences                                                    | High dimensionality, need lots of data, overfitting                                       |\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "slide_content_12 = \"\"\"\n",
        "\n",
        "## Fingertips of Fraud APSR by Francisco Cantu\n",
        "\n",
        "- You have 50,000 tallies from voting section, you think many can be forged/fraudulent\n",
        "\n",
        "- You can infer fraud by looking at unusual markings and deletions\n",
        "\n",
        "- You have two options:\n",
        "\n",
        "    - **manual** you or an assistant can spend time doing this\n",
        "\n",
        "    - **machine-learning** teach the computer to do it\n",
        "\n",
        "- Cantu chooses a combination: supervised (human-in-the-loop) machine learning to teach the machine how to classify ballots\n",
        "\n",
        "\"\"\"\n",
        "display(Markdown(slide_content_12))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "dFIeSMBJ2oXM",
        "outputId": "ddaa90b0-78ac-470f-956d-aa297d721125",
        "cellView": "form"
      },
      "id": "dFIeSMBJ2oXM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n## Fingertips of Fraud APSR by Francisco Cantu\n\n- You have 50,000 tallies from voting section, you think many can be forged/fraudulent  \n\n- You can infer fraud by looking at unusual markings and deletions\n\n- You have two options:\n\n    - **manual** you or an assistant can spend time doing this \n \n    - **machine-learning** teach the computer to do it \n\n- Cantu chooses a combination: supervised (human-in-the-loop) machine learning to teach the machine how to classify ballots\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_12cantu = \"\"\"\n",
        "\n",
        "## Cantu's Classification Approach\n",
        "\n",
        "- You take a random sample of several hundred images\n",
        "\n",
        "- You **label** them as fraud or clean:\n",
        "\n",
        "    - usually this means creating subfolders with **class1** and **class2** (...) images\n",
        "\n",
        "\t- You invoke a machine-learning model **ML** (something like a very non-transparent regression) in Python\n",
        "\n",
        "\t- You specify parameters such as:\n",
        "\n",
        "\t- how many images to train on and how many to use for validation/testing\n",
        "\n",
        "    - the latter means the computer sets aside some (often 20 per cent, random) labelled data and does not use it when learning\n",
        "\n",
        "    - the goal is to avoid overfitting, focusing on some irrelevant feature that predicts things in sample very well but makes the model less generalizable\n",
        "\n",
        "    - there are other parameters to set such as how many times to pass over the data (epochs), whether to resize, crop, revert to black and white\n",
        "\n",
        "\"\"\"\n",
        "display(Markdown(slide_content_12cantu))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "cellView": "form",
        "id": "hSVYzLyp6Xqj",
        "outputId": "ae7c8ced-634b-4195-adce-17b21b567856"
      },
      "id": "hSVYzLyp6Xqj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n## Cantu's Classification Approach\n\n- You take a random sample of several hundred images \n\n- You **label** them as fraud or clean:\n\n  - usually this means creating subfolders with **class1** and **class2** (...) images \n\n\t- You invoke a machine-learning model **ML** (something like a very non-transparent regression) in Python\n\n\t- You specify parameters such as: \n\n\t- how many images to train on and how many to use for validation/testing\n\n  - the latter means the computer sets aside some (often 20 per cent, random) labelled data and does not use it when learning \n\n  - the goal is to avoid overfitting, focusing on some irrelevant feature that predicts things in sample very well but makes the model less generalizable\n\n  - there are other parameters to set such as how many times to pass over the data (epochs), whether to resize, crop, revert to black and white\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_15 = \"\"\"\n",
        "\n",
        "project/                 # ðŸ‘ˆ Your overall project folder\n",
        "â”‚\n",
        "â”œâ”€â”€ data/                # ðŸ‘ˆ All your images (inputs to the ML pipeline)\n",
        "â”‚   â”œâ”€â”€ train/           # ðŸ‘ˆ Labeled training data (used to teach the model)\n",
        "â”‚   â”‚   â”œâ”€â”€ class1/      # e.g. \"cats\" â†’ contains only cat images\n",
        "â”‚   â”‚   â”œâ”€â”€ class2/      # e.g. \"dogs\" â†’ contains only dog images\n",
        "â”‚   â”‚   â””â”€â”€ ...          # more classes if needed\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ val/             # ðŸ‘ˆ Validation data (optional)\n",
        "â”‚   â”‚   â”œâ”€â”€ class1/      # same class structure as train/\n",
        "â”‚   â”‚   â”œâ”€â”€ class2/\n",
        "â”‚   â”‚   â””â”€â”€ ...\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ test/            # ðŸ‘ˆ Test data (optional)\n",
        "â”‚   â”‚   â”œâ”€â”€ class1/      # same structure again\n",
        "â”‚   â”‚   â”œâ”€â”€ class2/\n",
        "â”‚   â”‚   â””â”€â”€ ...\n",
        "â”‚   â”‚\n",
        "â”‚   â””â”€â”€ unlabeled/       # ðŸ‘ˆ Images you want predictions for\n",
        "â”‚       â”œâ”€â”€ img001.png   # no labels, just raw files\n",
        "â”‚       â”œâ”€â”€ img002.png\n",
        "â”‚       â””â”€â”€ ...\n",
        "â”‚\n",
        "â”œâ”€â”€ predictions/         # ðŸ‘ˆ Where you save model outputs\n",
        "â”‚   â”œâ”€â”€ unlabeled_results.csv   # e.g. file â†’ predicted class, probability\n",
        "â”‚   â””â”€â”€ labeled_examples.png    # optional visualization of results\n",
        "â”‚\n",
        "â”œâ”€â”€ src/                 # ðŸ‘ˆ Your source code\n",
        "â”‚   â”œâ”€â”€ train.py         # script to train the model\n",
        "â”‚   â”œâ”€â”€ evaluate.py      # script to test/evaluate the model\n",
        "â”‚   â”œâ”€â”€ predict.py       # script to classify new (unlabeled) images\n",
        "â”‚   â””â”€â”€ utils.py         # helper functions (loading, plotting, etc.)\n",
        "â”‚\n",
        "â””â”€â”€ notebooks/           # ðŸ‘ˆ Jupyter notebooks for experiments\n",
        "    â”œâ”€â”€ ex\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "KtqgcnFD44-n",
        "outputId": "d3aefd49-7f99-45d9-d06d-e834f9b55c60",
        "collapsed": true
      },
      "id": "KtqgcnFD44-n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nproject/                 # ðŸ‘ˆ Your overall project folder\nâ”‚\nâ”œâ”€â”€ data/                # ðŸ‘ˆ All your images (inputs to the ML pipeline)\nâ”‚   â”œâ”€â”€ train/           # ðŸ‘ˆ Labeled training data (used to teach the model)\nâ”‚   â”‚   â”œâ”€â”€ class1/      # e.g. \"cats\" â†’ contains only cat images\nâ”‚   â”‚   â”œâ”€â”€ class2/      # e.g. \"dogs\" â†’ contains only dog images\nâ”‚   â”‚   â””â”€â”€ ...          # more classes if needed\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ val/             # ðŸ‘ˆ Validation data (optional)\nâ”‚   â”‚   â”œâ”€â”€ class1/      # same class structure as train/\nâ”‚   â”‚   â”œâ”€â”€ class2/\nâ”‚   â”‚   â””â”€â”€ ...\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ test/            # ðŸ‘ˆ Test data (optional)\nâ”‚   â”‚   â”œâ”€â”€ class1/      # same structure again\nâ”‚   â”‚   â”œâ”€â”€ class2/\nâ”‚   â”‚   â””â”€â”€ ...\nâ”‚   â”‚\nâ”‚   â””â”€â”€ unlabeled/       # ðŸ‘ˆ Images you want predictions for\nâ”‚       â”œâ”€â”€ img001.png   # no labels, just raw files\nâ”‚       â”œâ”€â”€ img002.png\nâ”‚       â””â”€â”€ ...\nâ”‚\nâ”œâ”€â”€ predictions/         # ðŸ‘ˆ Where you save model outputs\nâ”‚   â”œâ”€â”€ unlabeled_results.csv   # e.g. file â†’ predicted class, probability\nâ”‚   â””â”€â”€ labeled_examples.png    # optional visualization of results\nâ”‚\nâ”œâ”€â”€ src/                 # ðŸ‘ˆ Your source code\nâ”‚   â”œâ”€â”€ train.py         # script to train the model\nâ”‚   â”œâ”€â”€ evaluate.py      # script to test/evaluate the model\nâ”‚   â”œâ”€â”€ predict.py       # script to classify new (unlabeled) images\nâ”‚   â””â”€â”€ utils.py         # helper functions (loading, plotting, etc.)\nâ”‚\nâ””â”€â”€ notebooks/           # ðŸ‘ˆ Jupyter notebooks for experiments\n    â”œâ”€â”€ ex\n\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_12cantu = \"\"\"\n",
        "\n",
        "## Cantu's Classification Approach\n",
        "\n",
        "- You take a random sample of several hundred images\n",
        "\n",
        "- You **label** them as fraud or clean:\n",
        "\n",
        "    - usually this means creating subfolders with **class1** and **class2** (...) images\n",
        "\n",
        "\t- You invoke a machine-learning model (something like a very non-transparent regression) in Python\n",
        "\n",
        "\t- You specify parameters such as:\n",
        "\n",
        "\t- how many images to train on and how many to use for validation/testing\n",
        "\n",
        "    - the latter means the computer sets aside some (often 20 per cent, random) labelled data and does not use it when learning\n",
        "\n",
        "    - the goal is to avoid overfitting, focusing on some irrelevant feature that predicts things in sample very well but makes the model less generalizable\n",
        "\n",
        "    - there are other parameters to set such as how many times to pass over the data (epochs), whether to resize, crop, revert to black and white\n",
        "\n",
        "\"\"\"\n",
        "display(Markdown(slide_content_12cantu))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "cellView": "form",
        "id": "-G4QH8ee6u4K",
        "outputId": "04e2d5da-6ab0-46a9-bc08-3e0e9bb218e4"
      },
      "id": "-G4QH8ee6u4K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n## Cantu's Classification Approach\n\n- You take a random sample of several hundred images \n\n- You **label** them as fraud or clean:\n\n    - usually this means creating subfolders with **class1** and **class2** (...) images \n\n\t- You invoke a machine-learning model (something like a very non-transparent regression) in Python\n\n\t- You specify parameters such as: \n\n\t- how many images to train on and how many to use for validation/testing\n\n    - the latter means the computer sets aside some (often 20 per cent, random) labelled data and does not use it when learning \n\n    - the goal is to avoid overfitting, focusing on some irrelevant feature that predicts things in sample very well but makes the model less generalizable\n\n    - there are other parameters to set such as how many times to pass over the data (epochs), whether to resize, crop, revert to black and white\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_12cantu3 = \"\"\"\n",
        "\n",
        "## Cantu's Result\n",
        "\n",
        "- The computer predicts/labels all 50 K images\n",
        "\n",
        "- You run a regression testing some theory of interest on which sections experience more fraud\n",
        "\n",
        "- Main takeaway - a human can do this, but computer is faster - by using ML we can do more research\n",
        "\n",
        "- Good use of data science (so do not use the tools just because you think they are cool and on data that does not matter - theory comes first)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_12cantu3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "cellView": "form",
        "id": "dMYf8MzbH_v0",
        "outputId": "0a13ab56-d931-4844-80e3-1982b22cc3ea"
      },
      "id": "dMYf8MzbH_v0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n## Cantu's Result\n\n- The computer predicts/labels all 50 K images \n\n- You run a regression testing some theory of interest on which sections experience more fraud\n\n- Main takeaway - a human can do this, but computer is faster - by using ML we can do more research\n\n- Good use of data science (so do not use the tools just because you think they are cool and on data that does not matter - theory comes first)\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_12cantu4 = \"\"\"\n",
        "\n",
        "## Types of learning\n",
        "\n",
        "- Cantu's paper shows supervised ML\n",
        "\n",
        "- You could run unsupervised models in which computer decides everying - discerning patters for you\n",
        "\n",
        "- The latter is problematic because human interpretation of the results tends to be post-hoc and hard to defend from a social science perspective\n",
        "\n",
        "- Some general problems - with all ML models, every time you run it, the result will differ (somewhat), no guarantee what would happen with different sample, why this model, why these options and so on\n",
        "\n",
        "- While **R** has some good native text-analysis tools, for images it is really **Py**\n",
        "\n",
        "- Ethical issues appear very fast - do you have permission to use the data, are you training models to discern racial features, what for and so on and so on\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "display(Markdown(slide_content_12cantu4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "cellView": "form",
        "id": "WLah0tqrIYaL",
        "outputId": "041f075a-af9c-4ba4-8e9f-a71f9ad64442"
      },
      "id": "WLah0tqrIYaL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n## Types of learning\n\n- Cantu's paper shows supervised ML \n\n- You could run unsupervised models in which computer decides everying - discerning patters for you\n\n- The latter is problematic because human interpretation of the results tends to be post-hoc and hard to defend from a social science perspective\n\n- Some general problems - with all ML models, every time you run it, the result will differ (somewhat), no guarantee what would happen with different sample, why this model, why these options and so on\n\n- While **R** has some good native text-analysis tools, for images it is really **Py** \n\n- Ethical issues appear very fast - do you have permission to use the data, are you training models to discern racial features, what for and so on and so on\n\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_ex = \"\"\"\n",
        "## We will download images and do three things\n",
        "\n",
        "- We will use the 500 social media banner images of current members of the European Parliament to ask\n",
        "    - Does the image contain faces?\n",
        "    - What other objects are in there?\n",
        "    - Do women use different images to communicate than men?\n",
        "\n",
        "- We will use Jupiter Notebook Py code available as Google Colab online notebook:\n",
        " \t- https://colab.research.google.com/drive/1PgwwMrvBnzJabIgQmuWqqTCabvEtJ_N5?usp=sharing\n",
        "\n",
        "- You need to upload a folder named ep_member_banner_img to your Google Drive:\n",
        " \t- the source folder is in GU OneDrive EP_MPs_BannerImages_FullSet\n",
        " \t- ep_member_banner_img should go in the main or root directory in Google Drive\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "cellView": "form",
        "id": "l46sc1qqK6BM",
        "outputId": "a47383c2-905b-4621-8828-bb998ebc2729"
      },
      "id": "l46sc1qqK6BM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## We will download images and do three things\n\n- We will use the 500 social media banner images of current members of the European Parliament to ask\n    - Does the image contain faces?\n    - What other objects are in there?\n    - Do women use different images to communicate than men? \n\n- We will use Jupiter Notebook Py code available as Google Colab online notebook:\n \t- https://colab.research.google.com/drive/1PgwwMrvBnzJabIgQmuWqqTCabvEtJ_N5?usp=sharing\n \n- You need to upload a folder named ep_member_banner_img to your Google Drive:\n \t- the source folder is in GU OneDrive EP_MPs_BannerImages_FullSet\n \t- ep_member_banner_img should go in the main or root directory\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ðŸ‘‡ USER: change this to the path of your copied dependency folder\n",
        "# You can get it by copying the link and making a shortcut or copy in your own Drive.\n",
        "# use this link to download the files and place them in ep_member_banner_img in root of Google drive:\n",
        "# https://drive.google.com/drive/folders/19eCSB-aPxrZoT9nRcxXenfleXdv9JrOg?usp=sharing\n",
        "#\n",
        "dependency_path = '/content/drive/MyDrive/ep_member_banner_img'\n",
        "\n",
        "print(f\"Dependency folder set to: {dependency_path}\")"
      ],
      "metadata": {
        "id": "NyjcGxF-PLYC"
      },
      "id": "NyjcGxF-PLYC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we import Google drive as virtual drive\n",
        "#we mount it, add operating system libary\n",
        "#we set the path where our files are\n",
        "#we create a list of the files and print to screen\n",
        "#this is just to make sure it all looks good\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/ep_member_banner_img'\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "print(\"Files in folder:\")\n",
        "for f in files:\n",
        "    print(f)"
      ],
      "metadata": {
        "id": "q3WtV8Rv46Yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2327e548-2917-427c-82db-22107bdfd63e"
      },
      "id": "q3WtV8Rv46Yf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files in folder:\n",
            "ep_m_aaltola.jpg\n",
            "ep_f_abadiÌajover.jpg\n",
            "ep_f_adamowicz.jpg\n",
            "ep_f_agirregoitiamartiÌnez.jpg\n",
            "ep_m_agius.jpg\n",
            "ep_m_agiussaliba.jpg\n",
            "ep_f_alsahlani.jpg\n",
            "ep_m_andresen.jpg\n",
            "ep_f_androueÌˆt.jpg\n",
            "ep_m_angel.jpg\n",
            "ep_m_antoci.jpg\n",
            "ep_m_ariasecheverriÌa.jpg\n",
            "ep_m_arimont.jpg\n",
            "ep_m_arÅ‚ukowicz.jpg\n",
            "ep_m_arnaoutoglou.jpg\n",
            "ep_m_assis.jpg\n",
            "ep_f_aubry.jpg\n",
            "ep_m_aust.jpg\n",
            "ep_m_ausÌŒtrevicÌŒius.jpg\n",
            "ep_m_azmani.jpg\n",
            "ep_f_ballariÌncereza.jpg\n",
            "ep_m_bardella.jpg\n",
            "ep_f_barley.jpg\n",
            "ep_m_bartulica.jpg\n",
            "ep_f_bartuÌŠsÌŒek.jpg\n",
            "ep_m_bay.jpg\n",
            "ep_m_beleris.jpg\n",
            "ep_m_bellamy.jpg\n",
            "ep_m_benea.jpg\n",
            "ep_m_benifei.jpg\n",
            "ep_f_benjumeabenjumea.jpg\n",
            "ep_f_benÌŒovaÌ.jpg\n",
            "ep_f_bentele.jpg\n",
            "ep_m_berendsen.jpg\n",
            "ep_m_berger.jpg\n",
            "ep_f_berg.jpg\n",
            "ep_m_berlato.jpg\n",
            "ep_m_bernhuber.jpg\n",
            "ep_m_biedronÌ.jpg\n",
            "ep_m_bielan.jpg\n",
            "ep_f_bischoff.jpg\n",
            "ep_m_bloss.jpg\n",
            "ep_m_boeselager.jpg\n",
            "ep_m_bogdan.jpg\n",
            "ep_f_bonte.jpg\n",
            "ep_m_borchia.jpg\n",
            "ep_f_borraÌspaboÌn.jpg\n",
            "ep_f_borvendeÌg.jpg\n",
            "ep_f_borzan.jpg\n",
            "ep_f_bosse.jpg\n",
            "ep_m_botenga.jpg\n",
            "ep_m_boyer.jpg\n",
            "ep_m_brejza.jpg\n",
            "ep_f_bricmont.jpg\n",
            "ep_f_brnjac.jpg\n",
            "ep_m_brudzinÌski.jpg\n",
            "ep_m_buda.jpg\n",
            "ep_m_attard.jpg\n",
            "ep_m_budka.jpg\n",
            "ep_m_bugalho.jpg\n",
            "ep_m_buÅ‚a.jpg\n",
            "ep_m_bullmann.jpg\n",
            "ep_f_burkhardt.jpg\n",
            "ep_m_buxadeÌvillalba.jpg\n",
            "ep_m_canfin.jpg\n",
            "ep_f_carberry.jpg\n",
            "ep_m_caÌ‚rciu.jpg\n",
            "ep_m_casa.jpg\n",
            "ep_m_caspary.jpg\n",
            "ep_m_cassart.jpg\n",
            "ep_m_castillo.jpg\n",
            "ep_f_cavazzini.jpg\n",
            "ep_f_ceccardi.jpg\n",
            "ep_m_cepeda.jpg\n",
            "ep_f_ceulemans.jpg\n",
            "ep_m_chahim.jpg\n",
            "ep_f_chaibi.jpg\n",
            "ep_m_chastel.jpg\n",
            "ep_f_chinnici.jpg\n",
            "ep_f_cifrovaÌostrihonÌŒovaÌ.jpg\n",
            "ep_f_cisint.jpg\n",
            "ep_m_clergeau.jpg\n",
            "ep_m_cormand.jpg\n",
            "ep_f_crespodiÌaz.jpg\n",
            "ep_m_cunha.jpg\n",
            "ep_m_dahl.jpg\n",
            "ep_m_danielsson.jpg\n",
            "ep_f_daÌvid.jpg\n",
            "ep_m_delahozquintano.jpg\n",
            "ep_m_dellavalle.jpg\n",
            "ep_f_deloge.jpg\n",
            "ep_m_demasi.jpg\n",
            "ep_m_demeo.jpg\n",
            "ep_f_demirel.jpg\n",
            "ep_m_deutsch.jpg\n",
            "ep_f_dieringer.jpg\n",
            "ep_m_diÌ‚ncu.jpg\n",
            "ep_f_disdier.jpg\n",
            "ep_f_dobrev.jpg\n",
            "ep_f_doherty.jpg\n",
            "ep_m_doleschal.jpg\n",
            "ep_m_dorfmann.jpg\n",
            "ep_m_dostaÌl.jpg\n",
            "ep_f_duÌˆpont.jpg\n",
            "ep_m_ecke.jpg\n",
            "ep_m_ehler.jpg\n",
            "ep_m_eickhout.jpg\n",
            "ep_m_erixon.jpg\n",
            "ep_f_estaraÌ€sferragut.jpg\n",
            "ep_f_ezcurraalmansa.jpg\n",
            "ep_m_falcaÌ†.jpg\n",
            "ep_m_falcone.jpg\n",
            "ep_m_beke.jpg\n",
            "ep_m_farantouris.jpg\n",
            "ep_f_farreng.jpg\n",
            "ep_m_farskyÌ.jpg\n",
            "ep_m_ferber.jpg\n",
            "ep_m_fernaÌndez.jpg\n",
            "ep_m_fidanza.jpg\n",
            "ep_f_firea.jpg\n",
            "ep_m_flanagan.jpg\n",
            "ep_m_fourlas.jpg\n",
            "ep_m_fragkos.jpg\n",
            "ep_m_freund.jpg\n",
            "ep_m_fiocchi.jpg\n",
            "ep_f_frigout.jpg\n",
            "ep_f_friis.jpg\n",
            "ep_f_fritzon.jpg\n",
            "ep_m_froelich.jpg\n",
            "ep_m_fuglsang.jpg\n",
            "ep_f_funchion.jpg\n",
            "ep_m_furore.jpg\n",
            "ep_m_gahler.jpg\n",
            "ep_f_galaÌn.jpg\n",
            "ep_f_gaÌlvez.jpg\n",
            "ep_f_garciÌapeÌrez.jpg\n",
            "ep_f_garciÌahermidavanderwalle.jpg\n",
            "ep_f_geese.jpg\n",
            "ep_f_gasiukpihowicz.jpg\n",
            "ep_m_geadi.jpg\n",
            "ep_f_gemma.jpg\n",
            "ep_m_germain.jpg\n",
            "ep_f_gerzsenyi.jpg\n",
            "ep_m_geuking.jpg\n",
            "ep_m_gieseke.jpg\n",
            "ep_m_gimeÌnezlarraz.jpg\n",
            "ep_f_glavak.jpg\n",
            "ep_m_gluÌˆck.jpg\n",
            "ep_m_glucksmann.jpg\n",
            "ep_m_goerens.jpg\n",
            "ep_m_gomart.jpg\n",
            "ep_f_goÌmezloÌpez.jpg\n",
            "ep_m_gonzaÌlezcasares.jpg\n",
            "ep_m_gonzaÌlezpons.jpg\n",
            "ep_m_gotink.jpg\n",
            "ep_m_gozi.jpg\n",
            "ep_f_grapini.jpg\n",
            "ep_f_gregorovaÌ.jpg\n",
            "ep_f_griset.jpg\n",
            "ep_f_gronkiewiczwaltz.jpg\n",
            "ep_m_groothuis.jpg\n",
            "ep_f_grossmann.jpg\n",
            "ep_m_grudler.jpg\n",
            "ep_f_gualmini.jpg\n",
            "ep_f_guarda.jpg\n",
            "ep_m_guetta.jpg\n",
            "ep_f_guzenina.jpg\n",
            "ep_f_gyoÌ‹ri.jpg\n",
            "ep_m_hadjipantela.jpg\n",
            "ep_f_hahn.jpg\n",
            "ep_m_halicki.jpg\n",
            "ep_m_hansen.jpg\n",
            "ep_f_hassan.jpg\n",
            "ep_m_hauser.jpg\n",
            "ep_m_haÌˆusling.jpg\n",
            "ep_m_hava.jpg\n",
            "ep_f_hayer.jpg\n",
            "ep_f_hazekamp.jpg\n",
            "ep_m_heide.jpg\n",
            "ep_m_heinaÌˆluoma.jpg\n",
            "ep_f_henriksson.jpg\n",
            "ep_m_herbst.jpg\n",
            "ep_m_hetman.jpg\n",
            "ep_f_hohlmeier.jpg\n",
            "ep_m_hojsiÌk.jpg\n",
            "ep_m_holmgren.jpg\n",
            "ep_m_hoÌˆlveÌnyi.jpg\n",
            "ep_f_homsginel.jpg\n",
            "ep_m_humberto.jpg\n",
            "ep_m_ijabs.jpg\n",
            "ep_f_imart.jpg\n",
            "ep_f_incir.jpg\n",
            "ep_f_jerkovicÌ.jpg\n",
            "ep_m_inselvini.jpg\n",
            "ep_f_jalloulmuro.jpg\n",
            "ep_m_jarubas.jpg\n",
            "ep_m_jonÌski.jpg\n",
            "ep_m_jouvet.jpg\n",
            "ep_f_joveva.jpg\n",
            "ep_m_kalfon.jpg\n",
            "ep_f_kalniete.jpg\n",
            "ep_m_kanev.jpg\n",
            "ep_f_karlsbro.jpg\n",
            "ep_m_kartheiser.jpg\n",
            "ep_f_karvasÌŒovaÌ.jpg\n",
            "ep_f_katainen.jpg\n",
            "ep_m_kefalogiannis.jpg\n",
            "ep_f_keller.jpg\n",
            "ep_m_kelly.jpg\n",
            "ep_f_kemp.jpg\n",
            "ep_m_kennes.jpg\n",
            "ep_f_khan.jpg\n",
            "ep_f_kircher.jpg\n",
            "ep_m_kobosko.jpg\n",
            "ep_m_koÌˆhler.jpg\n",
            "ep_m_kohut.jpg\n",
            "ep_f_kokalari.jpg\n",
            "ep_m_kolaÌrÌŒ.jpg\n",
            "ep_m_kols.jpg\n",
            "ep_f_konecÌŒnaÌ.jpg\n",
            "ep_f_kopacz.jpg\n",
            "ep_m_koÌˆrner.jpg\n",
            "ep_f_kountoura.jpg\n",
            "ep_m_kovatchev.jpg\n",
            "ep_m_krutiÌlek.jpg\n",
            "ep_m_kulja.jpg\n",
            "ep_m_kyuchyuk.jpg\n",
            "ep_m_lagodinsky.jpg\n",
            "ep_f_lakos.jpg\n",
            "ep_f_lalucq.jpg\n",
            "ep_m_lange.jpg\n",
            "ep_f_joron.jpg\n",
            "ep_m_knotek.jpg\n",
            "ep_f_jamet.jpg\n",
            "ep_f_juknevicÌŒieneÌ‡.jpg\n",
            "ep_f_langensiepen.jpg\n",
            "ep_m_laÌszloÌ.jpg\n",
            "ep_f_latinopoulou.jpg\n",
            "ep_f_lecallennec.jpg\n",
            "ep_m_leggeri.jpg\n",
            "ep_m_lenaers.jpg\n",
            "ep_m_leonardelli.jpg\n",
            "ep_m_lewandowski.jpg\n",
            "ep_f_lexmann.jpg\n",
            "ep_m_liese.jpg\n",
            "ep_m_lins.jpg\n",
            "ep_m_lÃ¸kkegaard.jpg\n",
            "ep_m_lopatka.jpg\n",
            "ep_m_loÌpez.jpg\n",
            "ep_m_loÌpezaguilar.jpg\n",
            "ep_m_loÌpezistuÌrizwhite.jpg\n",
            "ep_m_luena.jpg\n",
            "ep_f_Å‚ukacijewska.jpg\n",
            "ep_m_lupo.jpg\n",
            "ep_m_mcallister.jpg\n",
            "ep_m_madison.jpg\n",
            "ep_f_maestre.jpg\n",
            "ep_m_magyar.jpg\n",
            "ep_f_malaÌ¨g.jpg\n",
            "ep_m_mandl.jpg\n",
            "ep_m_maniatis.jpg\n",
            "ep_m_maran.jpg\n",
            "ep_f_marczuÅ‚ajtiswalczak.jpg\n",
            "ep_f_mareÌchal.jpg\n",
            "ep_m_mariani.jpg\n",
            "ep_m_marino.jpg\n",
            "ep_m_marquardt.jpg\n",
            "ep_m_martusciello.jpg\n",
            "ep_m_mato.jpg\n",
            "ep_f_matthieu.jpg\n",
            "ep_m_mavrides.jpg\n",
            "ep_f_maydell.jpg\n",
            "ep_m_mayer.jpg\n",
            "ep_m_mazurek.jpg\n",
            "ep_f_mebarek.jpg\n",
            "ep_f_mehnert.jpg\n",
            "ep_m_meimarakis.jpg\n",
            "ep_f_mendes.jpg\n",
            "ep_f_mendia.jpg\n",
            "ep_f_mertens.jpg\n",
            "ep_f_metsola.jpg\n",
            "ep_f_metz.jpg\n",
            "ep_m_milazzo.jpg\n",
            "ep_m_millaÌnmon.jpg\n",
            "ep_f_mirandapaz.jpg\n",
            "ep_m_molnaÌr.jpg\n",
            "ep_f_montserrat.jpg\n",
            "ep_f_morace.jpg\n",
            "ep_f_morano.jpg\n",
            "ep_f_moratti.jpg\n",
            "ep_f_moretti.jpg\n",
            "ep_m_mularczyk.jpg\n",
            "ep_m_muÌˆller.JPG\n",
            "ep_m_muresÌ§an.jpg\n",
            "ep_m_musÌ§oiu.jpg\n",
            "ep_m_nardella.jpg\n",
            "ep_m_negrescu.jpg\n",
            "ep_m_nemec.jpg\n",
            "ep_f_nerudovaÌ.jpg\n",
            "ep_f_neumann.jpg\n",
            "ep_f_nevadodelcampo.jpg\n",
            "ep_f_niebler.jpg\n",
            "ep_m_niedermayer.jpg\n",
            "ep_m_niinistoÌˆ.jpg\n",
            "ep_m_nikolic.jpg\n",
            "ep_f_niÌmhurchuÌ.jpg\n",
            "ep_f_noichl.jpg\n",
            "ep_m_novakov.jpg\n",
            "ep_f_nykiel.jpg\n",
            "ep_m_oetjen.jpg\n",
            "ep_f_ohisalo.jpg\n",
            "ep_m_omarjee.jpg\n",
            "ep_m_orlando.jpg\n",
            "ep_m_paet.jpg\n",
            "ep_f_palmisano.jpg\n",
            "ep_m_panayiotou.jpg\n",
            "ep_m_papandreou.jpg\n",
            "ep_f_paulus.jpg\n",
            "ep_m_pellerincarlin.jpg\n",
            "ep_m_peltier.jpg\n",
            "ep_f_pereira.jpg\n",
            "ep_f_peterhansen.jpg\n",
            "ep_m_picaro.jpg\n",
            "ep_f_picierno.jpg\n",
            "ep_m_picula.jpg\n",
            "ep_f_pietikaÌˆinen.jpg\n",
            "ep_m_popescu.jpg\n",
            "ep_f_princi.jpg\n",
            "ep_m_protas.jpg\n",
            "ep_m_puÌˆrner.jpg\n",
            "ep_m_radev.jpg\n",
            "ep_m_radtke.jpg\n",
            "ep_m_ratas.jpg\n",
            "ep_m_razza.jpg\n",
            "ep_f_rechagneux.jpg\n",
            "ep_f_regner.jpg\n",
            "ep_f_reintke.jpg\n",
            "ep_m_repasi.jpg\n",
            "ep_m_ressler.jpg\n",
            "ep_m_reuten.jpg\n",
            "ep_f_ribaiginer.jpg\n",
            "ep_f_ridel.jpg\n",
            "ep_f_ripa.jpg\n",
            "ep_m_rodrigues.jpg\n",
            "ep_m_rossempere.jpg\n",
            "ep_f_rothnevedÌŒalovaÌ.jpg\n",
            "ep_m_rougeÌ.jpg\n",
            "ep_m_ruissen.jpg\n",
            "ep_m_ruotolo.jpg\n",
            "ep_m_rzonÌca.jpg\n",
            "ep_m_salini.jpg\n",
            "ep_m_saÌnchezamor.jpg\n",
            "ep_m_sanchez.jpg\n",
            "ep_m_saramo.jpg\n",
            "ep_f_sardone.jpg\n",
            "ep_m_satouri.jpg\n",
            "ep_m_saudargas.jpg\n",
            "ep_f_schaldemose.jpg\n",
            "ep_m_schenk.jpg\n",
            "ep_f_scheuringwielgus.jpg\n",
            "ep_m_schieder.jpg\n",
            "ep_f_schneider.jpg\n",
            "ep_m_schwab.jpg\n",
            "ep_m_seekatz.jpg\n",
            "ep_f_serranosierra.jpg\n",
            "ep_m_sidl.jpg\n",
            "ep_m_sieper.jpg\n",
            "ep_m_simon.jpg\n",
            "ep_f_sippel.jpg\n",
            "ep_m_smit.jpg\n",
            "ep_m_sokol.jpg\n",
            "ep_f_soliÌspeÌrez.jpg\n",
            "ep_f_sommen.jpg\n",
            "ep_m_sonneborn.jpg\n",
            "ep_m_sousasilva.jpg\n",
            "ep_m_sÌ¦tefaÌ†nutÌ¦aÌ†.jpg\n",
            "ep_m_storm.jpg\n",
            "ep_m_stoÌˆteler.jpg\n",
            "ep_m_stoyanov.jpg\n",
            "ep_f_strackzimmermann.jpg\n",
            "ep_m_streit.jpg\n",
            "ep_f_steger.jpg\n",
            "ep_f_strik.jpg\n",
            "ep_f_strolenberg.jpg\n",
            "ep_m_sypniewski.jpg\n",
            "ep_m_szczerba.jpg\n",
            "ep_m_szydÅ‚o.jpg\n",
            "ep_m_tarczynÌski.jpg\n",
            "ep_f_tavares.jpg\n",
            "ep_m_tegethoff.jpg\n",
            "ep_f_teodorescumaÌŠwe.jpg\n",
            "ep_m_terhesÌ§.jpg\n",
            "ep_f_terlaak.jpg\n",
            "ep_m_terras.jpg\n",
            "ep_m_tertsch.jpg\n",
            "ep_m_thionnet.jpg\n",
            "ep_f_timgren.jpg\n",
            "ep_f_tinagli.jpg\n",
            "ep_m_tobeÌ.jpg\n",
            "ep_m_tomac.jpg\n",
            "ep_f_tomasÌŒicÌŒ.jpg\n",
            "ep_f_tomc.jpg\n",
            "ep_m_tonin.jpg\n",
            "ep_f_toom.jpg\n",
            "ep_m_torselli.jpg\n",
            "ep_m_tosi.jpg\n",
            "ep_f_toussaint.jpg\n",
            "ep_f_tovaglieri.JPG\n",
            "ep_m_toveri.jpg\n",
            "ep_m_tynkkynen.jpg\n",
            "ep_m_uhriÌk.jpg\n",
            "ep_f_vaidere.jpg\n",
            "ep_f_vaÌ†lean.jpg\n",
            "ep_m_valet.jpg\n",
            "ep_m_vandenberg.jpg\n",
            "ep_m_vandendriessche.jpg\n",
            "ep_m_vanlanschot.jpg\n",
            "ep_f_vanleeuwen.jpg\n",
            "ep_f_vautmans.jpg\n",
            "ep_f_vedrenne.jpg\n",
            "ep_f_verheyen.jpg\n",
            "ep_m_verougstraete\n",
            "ep_m_vesÌŒligaj.jpg\n",
            "ep_f_vieira.jpg\n",
            "ep_m_vilimsky.jpg\n",
            "ep_m_vincze.jpg\n",
            "ep_f_vind.jpg\n",
            "ep_m_vistisen.jpg\n",
            "ep_m_volgin.jpg\n",
            "ep_m_vondra.jpg\n",
            "ep_m_voss.jpg\n",
            "ep_f_vozembergvrionidi.jpg\n",
            "ep_f_vrecionovaÌ.jpg\n",
            "ep_m_vaÌzquezlaÌzara.jpg\n",
            "ep_m_waitz.jpg\n",
            "ep_f_walsh.jpg\n",
            "ep_f_walsmann.jpg\n",
            "ep_m_warborn.jpg\n",
            "ep_m_wawrykiewicz.jpg\n",
            "ep_f_wcisÅ‚o.jpg\n",
            "ep_m_weber.jpg\n",
            "ep_m_weimers.jpg\n",
            "ep_f_wechsler.jpg\n",
            "ep_f_wiesner.jpg\n",
            "ep_m_wiezik.jpg\n",
            "ep_f_wilmeÌ€s.jpg\n",
            "ep_m_winkler.jpg\n",
            "ep_f_winzig.jpg\n",
            "ep_f_wiselerlima.jpg\n",
            "ep_f_wisÌniewska.jpg\n",
            "ep_m_woÌˆlken.jpg\n",
            "ep_f_wolters.jpg\n",
            "ep_f_yar.jpg\n",
            "ep_f_yoncourtin.jpg\n",
            "ep_f_zajaÌ¨czkowskahernik.jpg\n",
            "ep_f_zalewska.jpg\n",
            "ep_m_zÌŒalimas.jpg\n",
            "ep_m_zan.jpg\n",
            "ep_m_zarzalejos.jpg\n",
            "ep_m_zdechovskyÌ.jpg\n",
            "ep_m_zdrojewski.jpg\n",
            "ep_m_zÅ‚otowski.jpg\n",
            "ep_m_zoidoaÌlvarez.jpg\n",
            "ep_f_zovko.jpg\n",
            "ep_m_zver.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_chat = \"\"\"\n",
        "## We can ask Chat GPT for the code\n",
        "\n",
        "- Pose your question while explaining clearly - verbose is good, e g:\n",
        "    - Write Python code to run in Google Colab that goes through a folder of images in my Google Drive, detects whether each image contains a human face, and prints out which files contain faces by saying filename (use filenames) contains face or does not contain face. Stop after checking 20 images to save on time. The folder in which my images sit is called ep_member_banner_img and is in the root or main directory.\n",
        "\n",
        "\n",
        "- It gave the following code which:\n",
        "\t- chose for us OpenCVâ€™s pre-trained face detector\n",
        " \t- loads, loops through images, checks whether they are in fact images, creates a counter, creates a list of length 1,2 for number of faces it finds, and if the list has at least one element, prints the message has face and does not have otherwise\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_chat))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "cellView": "form",
        "id": "h_KxzNDPIWzA",
        "outputId": "acc0e73c-53bd-4267-ee7e-2efcc3225677"
      },
      "id": "h_KxzNDPIWzA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## We can ask Chat GPT for the code\n\n- Pose your question while explaining clearly - verbose is good, e g:\n    - Write Python code to run in Google Colab that goes through a folder of images in my Google Drive, detects whether each image contains a human face, and prints out which files contain faces by saying filename (use filenames) contains face or does not contain face. Stop after checking 20 images to save on time. The folder in which my images sit is called ep_member_banner_img and is in the root or main directory. \n\n\n- It gave the following code which:\n\t- chose for us OpenCVâ€™s pre-trained face detector\n \t- loads, loops through images, checks whether they are in fact images, creates a counter, creates a list of length 1,2 for number of faces it finds, and if the list has at least one element, prints the message has face and does not have otherwise\n \n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Step 3: Path to your folder with images\n",
        "folder_path = '/content/drive/MyDrive/ep_member_banner_img'\n",
        "\n",
        "# Step 4: Load OpenCVâ€™s pretrained face detector\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Step 5: Loop through images and check for faces\n",
        "results = []\n",
        "count = 0\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    # Skip non-image files\n",
        "    if not (filename.lower().endswith('.jpg') or filename.lower().endswith('.png') or filename.lower().endswith('.jpeg')):\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(folder_path, filename)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    if img is None:\n",
        "        results.append((filename, \"could not load image\"))\n",
        "        continue\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        results.append((filename, \"contains face\"))\n",
        "    else:\n",
        "        results.append((filename, \"does not contain face\"))\n",
        "\n",
        "    count += 1\n",
        "    if count >= 20:\n",
        "        break  # stop after 20 files\n",
        "\n",
        "# Step 6: Print results\n",
        "for name, status in results:\n",
        "    print(f\"{name}: {status}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UitptG5m0fci",
        "outputId": "b4db4d3e-89ae-4b79-ec34-d8610910df20"
      },
      "id": "UitptG5m0fci",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ep_m_aaltola.jpg: contains face\n",
            "ep_f_abadiÌajover.jpg: contains face\n",
            "ep_f_adamowicz.jpg: contains face\n",
            "ep_f_agirregoitiamartiÌnez.jpg: does not contain face\n",
            "ep_m_agius.jpg: contains face\n",
            "ep_m_agiussaliba.jpg: contains face\n",
            "ep_f_alsahlani.jpg: contains face\n",
            "ep_m_andresen.jpg: does not contain face\n",
            "ep_f_androueÌˆt.jpg: does not contain face\n",
            "ep_m_angel.jpg: contains face\n",
            "ep_m_antoci.jpg: contains face\n",
            "ep_m_ariasecheverriÌa.jpg: does not contain face\n",
            "ep_m_arimont.jpg: contains face\n",
            "ep_m_arÅ‚ukowicz.jpg: contains face\n",
            "ep_m_arnaoutoglou.jpg: does not contain face\n",
            "ep_m_assis.jpg: does not contain face\n",
            "ep_f_aubry.jpg: does not contain face\n",
            "ep_m_aust.jpg: contains face\n",
            "ep_m_ausÌŒtrevicÌŒius.jpg: does not contain face\n",
            "ep_m_azmani.jpg: contains face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_chat2 = \"\"\"\n",
        "## Another request to Chat GPT\n",
        "\n",
        "- We pose the question similarly to before:\n",
        "    - Write Python code to run in Google Colab that goes through a folder of images in my Google Drive, detects whether each image contains objects, and prints out a list of file name, set of objects found. I do not want too many objects. The images are the banner pictures on social media of politicians. The folder in which my images sit is called ep_member_banner_img and is in the root or main directory. You can suggest a library of package.\n",
        "\n",
        "\n",
        "- It gave the following code which:\n",
        "\t- uses an AI model to recognize whatâ€™s in each image, and prints out the most likely things (labels) that the model â€œsees.â€\n",
        "\t- imports tools: os â€” helps find files in folders, PIL.Image â€” opens and reads image files, transformers.pipeline â€” loads a ready-made AI model for image recognition, sets the folder path, the code uses Hugging Faceâ€™s transformers library and loads a model called â€œgoogle/vit-base-patch16-224â€ (a Vision Transformer model), this model was trained to recognize objects, animals, people, etc., in images, tries to load the image in RGB format, if an image canâ€™t be opened (e.g., itâ€™s corrupted), it prints a warning and moves on\n",
        "\t- Runs the AI classifier, Sends the image through the Vision Transformer model., Asks for the top 5 guesses (â€œtop_k=5â€) of whatâ€™s in the picture, each guess includes a label (e.g., â€œperson,â€ â€œdog,â€ â€œsuit,â€ â€œflagâ€) and a confidence score (how sure the model is), filters and prints results, keeps only the labels where the model is at least 20% confident (score > 0.2)\n",
        "\t- Prints the filename and those labels.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_chat2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "cellView": "form",
        "id": "ifD2NJ1V3Ye7",
        "outputId": "27ac9e09-d33c-445f-ae70-2bfd43509095"
      },
      "id": "ifD2NJ1V3Ye7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Another request to Chat GPT \n\n- We pose the question similarly to before:\n    - Write Python code to run in Google Colab that goes through a folder of images in my Google Drive, detects whether each image contains objects, and prints out a list of file name, set of objects found. I do not want too many objects. The images are the banner pictures on social media of politicians. The folder in which my images sit is called ep_member_banner_img and is in the root or main directory. You can suggest a library of package.\n\n\n- It gave the following code which:\n\t- uses an AI model to recognize whatâ€™s in each image, and prints out the most likely things (labels) that the model â€œsees.â€\n\t- imports tools: os â€” helps find files in folders, PIL.Image â€” opens and reads image files, transformers.pipeline â€” loads a ready-made AI model for image recognition, sets the folder path, the code uses Hugging Faceâ€™s transformers library and loads a model called â€œgoogle/vit-base-patch16-224â€ (a Vision Transformer model), this model was trained to recognize objects, animals, people, etc., in images, tries to load the image in RGB format, if an image canâ€™t be opened (e.g., itâ€™s corrupted), it prints a warning and moves on\n\t- Runs the AI classifier, Sends the image through the Vision Transformer model., Asks for the top 5 guesses (â€œtop_k=5â€) of whatâ€™s in the picture, each guess includes a label (e.g., â€œperson,â€ â€œdog,â€ â€œsuit,â€ â€œflagâ€) and a confidence score (how sure the model is), filters and prints results, keeps only the labels where the model is at least 20% confident (score > 0.2)\n\t- Prints the filename and those labels.\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import pipeline\n",
        "\n",
        "# Path to your folder of images\n",
        "folder_path = '/content/drive/MyDrive/ep_member_banner_img'\n",
        "\n",
        "# Load pretrained image classification pipeline\n",
        "classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
        "\n",
        "# Loop over files and classify\n",
        "for filename in os.listdir(folder_path):\n",
        "    if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "    except:\n",
        "        print(f\"âš ï¸ Could not open {filename}\")\n",
        "        continue\n",
        "\n",
        "    # Run classification (top 5 predictions)\n",
        "    preds = classifier(image, top_k=5)\n",
        "\n",
        "    # Collect label names above a confidence threshold\n",
        "    recognized = [p[\"label\"] for p in preds if p[\"score\"] > 0.2]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{filename}: {', '.join(recognized)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c0edcf64b1d3414d84915a665c4f98b8",
            "a17b9f4e084b42ada9476a4facd438f1",
            "77bd4440f3824e969b34684ad97c5a84",
            "5537e89b61004f1e97692d20bdef72d3",
            "3dcd60fdbcd24c3da3b14e6cc0f3829f",
            "b6eb39f977044792a7b698aafe2c1419",
            "ad8c311af5074760bb56bb67911fe2bf",
            "fb423a2525314a1e9db4e6d24026f677",
            "062a366822e146cf9a77d02d066705d8",
            "cfaa90899ef44a3db728447f4ab67a14",
            "7b759829486a49e6aac74a390a8f91f5",
            "1fda9499750f4dbe89b237524a4ee1d1",
            "1ccd26f9dab54a979be73357287e58cd",
            "60b8a83cd84441ab826efa71de307384",
            "fea1afae4f3746f3a785734fa7c4b9d6",
            "a2121a2ee6b94e4495af315dd2e08c73",
            "99c38abfd9af48e0b9f95f9f75adbbb5",
            "95cffa6a168b446b99e3ff652b84052f",
            "5bbe170f4a114f6c83ab122f58e9f28d",
            "97c8dc2f5dac4ab891243a2d0f5c7fe1",
            "87f8c71866034023a2e2d6c5348462c5",
            "b7d14bc651374a76b622705f02d23b6f",
            "78c148ebfc1b4a1eb3cbe21c36669d4d",
            "edaa64f5d9c245d5b60958d603261afc",
            "49c71f12d7434b80ba8d8fe0c2787e31",
            "95af6ea5d426444b873f8ef1f26c9685",
            "c49cbb0e418d4748ba18434f5b3f472a",
            "97ea37b1933b4e03ad5f96dabb40ab72",
            "4f79d9f71199463b907ec50c3f1c4343",
            "a475e276a5a341e98b5c5015939fbb10",
            "c462b49712ce4af19b32ac39f0aa71f7",
            "e2ed3f2d7be84a54a5fb29098c63053b",
            "f45be24ed13d4c379639b73854ac2c53"
          ]
        },
        "id": "8kUOZLVC9npq",
        "outputId": "cf7b3baa-2c80-4666-8330-b23d22b5f5c7",
        "cellView": "form"
      },
      "id": "8kUOZLVC9npq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0edcf64b1d3414d84915a665c4f98b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fda9499750f4dbe89b237524a4ee1d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78c148ebfc1b4a1eb3cbe21c36669d4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep_m_aaltola.jpg: suit, suit of clothes\n",
            "ep_f_abadiÌajover.jpg: academic gown, academic robe, judge's robe, mortarboard\n",
            "ep_f_adamowicz.jpg: vestment, academic gown, academic robe, judge's robe\n",
            "ep_f_agirregoitiamartiÌnez.jpg: flagpole, flagstaff\n",
            "ep_m_agius.jpg: web site, website, internet site, site\n",
            "ep_m_agiussaliba.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_f_alsahlani.jpg: web site, website, internet site, site\n",
            "ep_m_andresen.jpg: balance beam, beam\n",
            "ep_f_androueÌˆt.jpg: peacock\n",
            "ep_m_angel.jpg: theater curtain, theatre curtain\n",
            "ep_m_antoci.jpg: \n",
            "ep_m_ariasecheverriÌa.jpg: drilling platform, offshore rig\n",
            "ep_m_arimont.jpg: suit, suit of clothes\n",
            "ep_m_arÅ‚ukowicz.jpg: \n",
            "ep_m_arnaoutoglou.jpg: pier\n",
            "ep_m_assis.jpg: \n",
            "ep_f_aubry.jpg: stage, theater curtain, theatre curtain\n",
            "ep_m_aust.jpg: desktop computer, notebook, notebook computer\n",
            "ep_m_ausÌŒtrevicÌŒius.jpg: flagpole, flagstaff\n",
            "ep_m_azmani.jpg: restaurant, eating house, eating place, eatery, notebook, notebook computer\n",
            "ep_f_ballariÌncereza.jpg: steel drum\n",
            "ep_m_bardella.jpg: suit, suit of clothes\n",
            "ep_f_barley.jpg: lakeside, lakeshore\n",
            "ep_m_bartulica.jpg: planetarium, cinema, movie theater, movie theatre, movie house, picture palace\n",
            "ep_f_bartuÌŠsÌŒek.jpg: \n",
            "ep_m_bay.jpg: suit, suit of clothes\n",
            "ep_m_beleris.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_m_bellamy.jpg: stage\n",
            "ep_m_benea.jpg: digital clock\n",
            "ep_m_benifei.jpg: web site, website, internet site, site\n",
            "ep_f_benjumeabenjumea.jpg: \n",
            "ep_f_benÌŒovaÌ.jpg: buckle, shield, buckler\n",
            "ep_f_bentele.jpg: pole\n",
            "ep_m_berendsen.jpg: lab coat, laboratory coat\n",
            "ep_m_berger.jpg: web site, website, internet site, site\n",
            "ep_f_berg.jpg: suit, suit of clothes\n",
            "ep_m_berlato.jpg: scoreboard\n",
            "ep_m_bernhuber.jpg: \n",
            "ep_m_biedronÌ.jpg: web site, website, internet site, site\n",
            "ep_m_bielan.jpg: Windsor tie\n",
            "ep_f_bischoff.jpg: \n",
            "ep_m_bloss.jpg: streetcar, tram, tramcar, trolley, trolley car\n",
            "ep_m_boeselager.jpg: stage, academic gown, academic robe, judge's robe\n",
            "ep_m_bogdan.jpg: web site, website, internet site, site\n",
            "ep_f_bonte.jpg: maze, labyrinth\n",
            "ep_m_borchia.jpg: maze, labyrinth\n",
            "ep_f_borraÌspaboÌn.jpg: torch\n",
            "ep_f_borvendeÌg.jpg: web site, website, internet site, site, envelope\n",
            "ep_f_borzan.jpg: torch\n",
            "ep_f_bosse.jpg: \n",
            "ep_m_botenga.jpg: jersey, T-shirt, tee shirt\n",
            "ep_m_boyer.jpg: web site, website, internet site, site\n",
            "ep_m_brejza.jpg: moving van, brass, memorial tablet, plaque\n",
            "ep_f_bricmont.jpg: envelope\n",
            "ep_f_brnjac.jpg: web site, website, internet site, site\n",
            "ep_m_brudzinÌski.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_m_buda.jpg: stage, cinema, movie theater, movie theatre, movie house, picture palace, theater curtain, theatre curtain\n",
            "ep_m_attard.jpg: promontory, headland, head, foreland\n",
            "ep_m_budka.jpg: seashore, coast, seacoast, sea-coast, lakeside, lakeshore\n",
            "ep_m_bugalho.jpg: \n",
            "ep_m_buÅ‚a.jpg: \n",
            "ep_m_bullmann.jpg: flagpole, flagstaff\n",
            "ep_f_burkhardt.jpg: web site, website, internet site, site\n",
            "ep_m_buxadeÌvillalba.jpg: \n",
            "ep_m_canfin.jpg: valley, vale, lakeside, lakeshore\n",
            "ep_f_carberry.jpg: web site, website, internet site, site\n",
            "ep_m_caÌ‚rciu.jpg: panpipe, pandean pipe, syrinx\n",
            "ep_m_casa.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_m_caspary.jpg: \n",
            "ep_m_cassart.jpg: \n",
            "ep_m_castillo.jpg: spotlight, spot\n",
            "ep_f_cavazzini.jpg: \n",
            "ep_f_ceccardi.jpg: lab coat, laboratory coat\n",
            "ep_m_cepeda.jpg: \n",
            "ep_f_ceulemans.jpg: \n",
            "ep_m_chahim.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_chaibi.jpg: web site, website, internet site, site\n",
            "ep_m_chastel.jpg: planetarium\n",
            "ep_f_chinnici.jpg: cinema, movie theater, movie theatre, movie house, picture palace\n",
            "ep_f_cifrovaÌostrihonÌŒovaÌ.jpg: \n",
            "ep_f_cisint.jpg: harmonica, mouth organ, harp, mouth harp\n",
            "ep_m_clergeau.jpg: suit, suit of clothes\n",
            "ep_m_cormand.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_f_crespodiÌaz.jpg: suit, suit of clothes\n",
            "ep_m_cunha.jpg: analog clock\n",
            "ep_m_dahl.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_m_danielsson.jpg: ambulance\n",
            "ep_f_daÌvid.jpg: \n",
            "ep_m_delahozquintano.jpg: \n",
            "ep_m_dellavalle.jpg: web site, website, internet site, site\n",
            "ep_f_deloge.jpg: \n",
            "ep_m_demasi.jpg: pickup, pickup truck\n",
            "ep_m_demeo.jpg: web site, website, internet site, site\n",
            "ep_f_demirel.jpg: cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\n",
            "ep_m_deutsch.jpg: flagpole, flagstaff, palace\n",
            "ep_f_dieringer.jpg: lakeside, lakeshore\n",
            "ep_m_diÌ‚ncu.jpg: flagpole, flagstaff\n",
            "ep_f_disdier.jpg: cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\n",
            "ep_f_dobrev.jpg: book jacket, dust cover, dust jacket, dust wrapper, comic book\n",
            "ep_f_doherty.jpg: \n",
            "ep_m_doleschal.jpg: \n",
            "ep_m_dorfmann.jpg: web site, website, internet site, site\n",
            "ep_m_dostaÌl.jpg: \n",
            "ep_f_duÌˆpont.jpg: web site, website, internet site, site, suit, suit of clothes\n",
            "ep_m_ecke.jpg: lakeside, lakeshore\n",
            "ep_m_ehler.jpg: flagpole, flagstaff\n",
            "ep_m_eickhout.jpg: flute, transverse flute, oboe, hautboy, hautbois\n",
            "ep_m_erixon.jpg: \n",
            "ep_f_estaraÌ€sferragut.jpg: bathing cap, swimming cap\n",
            "ep_f_ezcurraalmansa.jpg: \n",
            "ep_m_falcaÌ†.jpg: web site, website, internet site, site\n",
            "ep_m_falcone.jpg: web site, website, internet site, site\n",
            "ep_m_beke.jpg: web site, website, internet site, site\n",
            "ep_m_farantouris.jpg: prison, prison house\n",
            "ep_f_farreng.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_farskyÌ.jpg: jersey, T-shirt, tee shirt\n",
            "ep_m_ferber.jpg: flagpole, flagstaff\n",
            "ep_m_fernaÌndez.jpg: \n",
            "ep_m_fidanza.jpg: web site, website, internet site, site\n",
            "ep_f_firea.jpg: nematode, nematode worm, roundworm\n",
            "ep_m_flanagan.jpg: \n",
            "ep_m_fourlas.jpg: palace, planetarium\n",
            "ep_m_fragkos.jpg: web site, website, internet site, site\n",
            "ep_m_freund.jpg: \n",
            "ep_m_fiocchi.jpg: \n",
            "ep_f_frigout.jpg: \n",
            "ep_f_friis.jpg: \n",
            "ep_f_fritzon.jpg: \n",
            "ep_m_froelich.jpg: television, television system\n",
            "ep_m_fuglsang.jpg: \n",
            "ep_f_funchion.jpg: cinema, movie theater, movie theatre, movie house, picture palace\n",
            "ep_m_furore.jpg: web site, website, internet site, site\n",
            "ep_m_gahler.jpg: \n",
            "ep_f_galaÌn.jpg: \n",
            "ep_f_gaÌlvez.jpg: wardrobe, closet, press\n",
            "ep_f_garciÌapeÌrez.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_garciÌahermidavanderwalle.jpg: flagpole, flagstaff\n",
            "ep_f_geese.jpg: umbrella\n",
            "ep_f_gasiukpihowicz.jpg: \n",
            "ep_m_geadi.jpg: analog clock\n",
            "ep_f_gemma.jpg: web site, website, internet site, site\n",
            "ep_m_germain.jpg: theater curtain, theatre curtain\n",
            "ep_f_gerzsenyi.jpg: pick, plectrum, plectron\n",
            "ep_m_geuking.jpg: \n",
            "ep_m_gieseke.jpg: web site, website, internet site, site\n",
            "ep_m_gimeÌnezlarraz.jpg: flagpole, flagstaff\n",
            "ep_f_glavak.jpg: suit, suit of clothes\n",
            "ep_m_gluÌˆck.jpg: flagpole, flagstaff\n",
            "ep_m_glucksmann.jpg: \n",
            "ep_m_goerens.jpg: palace\n",
            "ep_m_gomart.jpg: alp, valley, vale\n",
            "ep_f_goÌmezloÌpez.jpg: bell cote, bell cot\n",
            "ep_m_gonzaÌlezcasares.jpg: \n",
            "ep_m_gonzaÌlezpons.jpg: abaya\n",
            "ep_m_gotink.jpg: suit, suit of clothes\n",
            "ep_m_gozi.jpg: screen, CRT screen\n",
            "ep_f_grapini.jpg: dome\n",
            "ep_f_gregorovaÌ.jpg: \n",
            "ep_f_griset.jpg: web site, website, internet site, site\n",
            "ep_f_gronkiewiczwaltz.jpg: lakeside, lakeshore\n",
            "ep_m_groothuis.jpg: academic gown, academic robe, judge's robe, mortarboard\n",
            "ep_f_grossmann.jpg: stage\n",
            "ep_m_grudler.jpg: \n",
            "ep_f_gualmini.jpg: \n",
            "ep_f_guarda.jpg: \n",
            "ep_m_guetta.jpg: \n",
            "ep_f_guzenina.jpg: \n",
            "ep_f_gyoÌ‹ri.jpg: \n",
            "ep_m_hadjipantela.jpg: web site, website, internet site, site\n",
            "ep_f_hahn.jpg: \n",
            "ep_m_halicki.jpg: web site, website, internet site, site\n",
            "ep_m_hansen.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_f_hassan.jpg: schooner\n",
            "ep_m_hauser.jpg: web site, website, internet site, site\n",
            "ep_m_haÌˆusling.jpg: valley, vale\n",
            "ep_m_hava.jpg: jersey, T-shirt, tee shirt\n",
            "ep_f_hayer.jpg: \n",
            "ep_f_hazekamp.jpg: \n",
            "ep_m_heide.jpg: suit, suit of clothes\n",
            "ep_m_heinaÌˆluoma.jpg: bassoon\n",
            "ep_f_henriksson.jpg: suit, suit of clothes\n",
            "ep_m_herbst.jpg: \n",
            "ep_m_hetman.jpg: \n",
            "ep_f_hohlmeier.jpg: television, television system\n",
            "ep_m_hojsiÌk.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_holmgren.jpg: lab coat, laboratory coat\n",
            "ep_m_hoÌˆlveÌnyi.jpg: \n",
            "ep_f_homsginel.jpg: moving van\n",
            "ep_m_humberto.jpg: \n",
            "ep_m_ijabs.jpg: suit, suit of clothes\n",
            "ep_f_imart.jpg: hay\n",
            "ep_f_incir.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_jerkovicÌ.jpg: cinema, movie theater, movie theatre, movie house, picture palace\n",
            "ep_m_inselvini.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_f_jalloulmuro.jpg: \n",
            "ep_m_jarubas.jpg: can opener, tin opener\n",
            "ep_m_jonÌski.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_m_jouvet.jpg: suit, suit of clothes\n",
            "ep_f_joveva.jpg: bow\n",
            "ep_m_kalfon.jpg: space shuttle\n",
            "ep_f_kalniete.jpg: \n",
            "ep_m_kanev.jpg: \n",
            "ep_f_karlsbro.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_m_kartheiser.jpg: flagpole, flagstaff\n",
            "ep_f_karvasÌŒovaÌ.jpg: pajama, pyjama, pj's, jammies\n",
            "ep_f_katainen.jpg: \n",
            "ep_m_kefalogiannis.jpg: web site, website, internet site, site\n",
            "ep_f_keller.jpg: \n",
            "ep_m_kelly.jpg: web site, website, internet site, site\n",
            "ep_f_kemp.jpg: \n",
            "ep_m_kennes.jpg: \n",
            "ep_f_khan.jpg: \n",
            "ep_f_kircher.jpg: alp\n",
            "ep_m_kobosko.jpg: mortarboard\n",
            "ep_m_koÌˆhler.jpg: ox\n",
            "ep_m_kohut.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_f_kokalari.jpg: notebook, notebook computer\n",
            "ep_m_kolaÌrÌŒ.jpg: rule, ruler\n",
            "ep_m_kols.jpg: web site, website, internet site, site\n",
            "ep_f_konecÌŒnaÌ.jpg: \n",
            "ep_f_kopacz.jpg: lab coat, laboratory coat, web site, website, internet site, site\n",
            "ep_m_koÌˆrner.jpg: library, loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\n",
            "ep_f_kountoura.jpg: abacus\n",
            "ep_m_kovatchev.jpg: flagpole, flagstaff, palace\n",
            "ep_m_krutiÌlek.jpg: comic book, web site, website, internet site, site, book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_m_kulja.jpg: stethoscope\n",
            "ep_m_kyuchyuk.jpg: \n",
            "ep_m_lagodinsky.jpg: \n",
            "ep_f_lakos.jpg: \n",
            "ep_f_lalucq.jpg: \n",
            "ep_m_lange.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_f_joron.jpg: web site, website, internet site, site\n",
            "ep_m_knotek.jpg: \n",
            "ep_f_jamet.jpg: \n",
            "ep_f_juknevicÌŒieneÌ‡.jpg: sulphur butterfly, sulfur butterfly\n",
            "ep_f_langensiepen.jpg: \n",
            "ep_m_laÌszloÌ.jpg: stage\n",
            "ep_f_latinopoulou.jpg: screen, CRT screen, desktop computer\n",
            "ep_f_lecallennec.jpg: castle\n",
            "ep_m_leggeri.jpg: academic gown, academic robe, judge's robe, mortarboard\n",
            "ep_m_lenaers.jpg: \n",
            "ep_m_leonardelli.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_lewandowski.jpg: web site, website, internet site, site\n",
            "ep_f_lexmann.jpg: web site, website, internet site, site\n",
            "ep_m_liese.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_m_lins.jpg: harvester, reaper, thresher, thrasher, threshing machine\n",
            "ep_m_lÃ¸kkegaard.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_m_lopatka.jpg: \n",
            "ep_m_loÌpez.jpg: \n",
            "ep_m_loÌpezaguilar.jpg: suit, suit of clothes\n",
            "ep_m_loÌpezistuÌrizwhite.jpg: jersey, T-shirt, tee shirt\n",
            "ep_m_luena.jpg: \n",
            "ep_f_Å‚ukacijewska.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_lupo.jpg: planetarium, scoreboard\n",
            "ep_m_mcallister.jpg: web site, website, internet site, site\n",
            "ep_m_madison.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_f_maestre.jpg: flagpole, flagstaff, maze, labyrinth\n",
            "ep_m_magyar.jpg: \n",
            "ep_f_malaÌ¨g.jpg: web site, website, internet site, site\n",
            "ep_m_mandl.jpg: \n",
            "ep_m_maniatis.jpg: Windsor tie\n",
            "ep_m_maran.jpg: monitor, screen, CRT screen\n",
            "ep_f_marczuÅ‚ajtiswalczak.jpg: web site, website, internet site, site\n",
            "ep_f_mareÌchal.jpg: overskirt\n",
            "ep_m_mariani.jpg: \n",
            "ep_m_marino.jpg: scoreboard\n",
            "ep_m_marquardt.jpg: \n",
            "ep_m_martusciello.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_m_mato.jpg: web site, website, internet site, site\n",
            "ep_f_matthieu.jpg: monitor, screen, CRT screen\n",
            "ep_m_mavrides.jpg: \n",
            "ep_f_maydell.jpg: suit, suit of clothes\n",
            "ep_m_mayer.jpg: web site, website, internet site, site\n",
            "ep_m_mazurek.jpg: Windsor tie, suit, suit of clothes\n",
            "ep_f_mebarek.jpg: web site, website, internet site, site\n",
            "ep_f_mehnert.jpg: \n",
            "ep_m_meimarakis.jpg: \n",
            "ep_f_mendes.jpg: steel drum\n",
            "ep_f_mendia.jpg: scoreboard, cinema, movie theater, movie theatre, movie house, picture palace\n",
            "ep_f_mertens.jpg: tobacco shop, tobacconist shop, tobacconist\n",
            "ep_f_metsola.jpg: \n",
            "ep_f_metz.jpg: parachute, chute, flagpole, flagstaff\n",
            "ep_m_milazzo.jpg: doormat, welcome mat\n",
            "ep_m_millaÌnmon.jpg: lakeside, lakeshore, yawl\n",
            "ep_f_mirandapaz.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_molnaÌr.jpg: lab coat, laboratory coat, web site, website, internet site, site\n",
            "ep_f_montserrat.jpg: \n",
            "ep_f_morace.jpg: tennis ball\n",
            "ep_f_morano.jpg: \n",
            "ep_f_moratti.jpg: suit, suit of clothes\n",
            "ep_f_moretti.jpg: web site, website, internet site, site\n",
            "ep_m_mularczyk.jpg: mortarboard\n",
            "ep_m_muÌˆller.JPG: web site, website, internet site, site\n",
            "ep_m_muresÌ§an.jpg: suit, suit of clothes, web site, website, internet site, site\n",
            "ep_m_musÌ§oiu.jpg: \n",
            "ep_m_nardella.jpg: mailbox, letter box\n",
            "ep_m_negrescu.jpg: web site, website, internet site, site\n",
            "ep_m_nemec.jpg: book jacket, dust cover, dust jacket, dust wrapper, comic book\n",
            "ep_f_nerudovaÌ.jpg: \n",
            "ep_f_neumann.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_nevadodelcampo.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_niebler.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_m_niedermayer.jpg: torch\n",
            "ep_m_niinistoÌˆ.jpg: \n",
            "ep_m_nikolic.jpg: suit, suit of clothes\n",
            "ep_f_niÌmhurchuÌ.jpg: web site, website, internet site, site\n",
            "ep_f_noichl.jpg: dam, dike, dyke, lakeside, lakeshore\n",
            "ep_m_novakov.jpg: \n",
            "ep_f_nykiel.jpg: flagpole, flagstaff\n",
            "ep_m_oetjen.jpg: nail\n",
            "ep_f_ohisalo.jpg: \n",
            "ep_m_omarjee.jpg: electric fan, blower\n",
            "ep_m_orlando.jpg: web site, website, internet site, site\n",
            "ep_m_paet.jpg: tripod\n",
            "ep_f_palmisano.jpg: scale, weighing machine\n",
            "ep_m_panayiotou.jpg: scoreboard\n",
            "ep_m_papandreou.jpg: web site, website, internet site, site\n",
            "ep_f_paulus.jpg: web site, website, internet site, site\n",
            "ep_m_pellerincarlin.jpg: scoreboard\n",
            "ep_m_peltier.jpg: suit, suit of clothes\n",
            "ep_f_pereira.jpg: \n",
            "ep_f_peterhansen.jpg: maypole\n",
            "ep_m_picaro.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_f_picierno.jpg: \n",
            "ep_m_picula.jpg: promontory, headland, head, foreland, seashore, coast, seacoast, sea-coast\n",
            "ep_f_pietikaÌˆinen.jpg: \n",
            "ep_m_popescu.jpg: \n",
            "ep_f_princi.jpg: suit, suit of clothes\n",
            "ep_m_protas.jpg: web site, website, internet site, site\n",
            "ep_m_puÌˆrner.jpg: \n",
            "ep_m_radev.jpg: torch\n",
            "ep_m_radtke.jpg: web site, website, internet site, site\n",
            "ep_m_ratas.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_m_razza.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_f_rechagneux.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_regner.jpg: web site, website, internet site, site\n",
            "ep_f_reintke.jpg: trench coat\n",
            "ep_m_repasi.jpg: planetarium\n",
            "ep_m_ressler.jpg: web site, website, internet site, site\n",
            "ep_m_reuten.jpg: \n",
            "ep_f_ribaiginer.jpg: street sign\n",
            "ep_f_ridel.jpg: \n",
            "ep_f_ripa.jpg: miniskirt, mini\n",
            "ep_m_rodrigues.jpg: web site, website, internet site, site\n",
            "ep_m_rossempere.jpg: \n",
            "ep_f_rothnevedÌŒalovaÌ.jpg: \n",
            "ep_m_rougeÌ.jpg: web site, website, internet site, site\n",
            "ep_m_ruissen.jpg: \n",
            "ep_m_ruotolo.jpg: torch, plunger, plumber's helper\n",
            "ep_m_rzonÌca.jpg: comic book\n",
            "ep_m_salini.jpg: stage\n",
            "ep_m_saÌnchezamor.jpg: shower curtain\n",
            "ep_m_sanchez.jpg: seashore, coast, seacoast, sea-coast, dock, dockage, docking facility\n",
            "ep_m_saramo.jpg: Band Aid\n",
            "ep_f_sardone.jpg: web site, website, internet site, site\n",
            "ep_m_satouri.jpg: \n",
            "ep_m_saudargas.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_f_schaldemose.jpg: \n",
            "ep_m_schenk.jpg: web site, website, internet site, site\n",
            "ep_f_scheuringwielgus.jpg: \n",
            "ep_m_schieder.jpg: suit, suit of clothes, web site, website, internet site, site\n",
            "ep_f_schneider.jpg: flagpole, flagstaff\n",
            "ep_m_schwab.jpg: Windsor tie, suit, suit of clothes\n",
            "ep_m_seekatz.jpg: flagpole, flagstaff\n",
            "ep_f_serranosierra.jpg: theater curtain, theatre curtain\n",
            "ep_m_sidl.jpg: \n",
            "ep_m_sieper.jpg: \n",
            "ep_m_simon.jpg: web site, website, internet site, site\n",
            "ep_f_sippel.jpg: envelope\n",
            "ep_m_smit.jpg: clog, geta, patten, sabot\n",
            "ep_m_sokol.jpg: \n",
            "ep_f_soliÌspeÌrez.jpg: promontory, headland, head, foreland, cliff, drop, drop-off\n",
            "ep_f_sommen.jpg: pajama, pyjama, pj's, jammies\n",
            "ep_m_sonneborn.jpg: \n",
            "ep_m_sousasilva.jpg: lab coat, laboratory coat\n",
            "ep_m_sÌ¦tefaÌ†nutÌ¦aÌ†.jpg: \n",
            "ep_m_storm.jpg: trench coat\n",
            "ep_m_stoÌˆteler.jpg: power drill\n",
            "ep_m_stoyanov.jpg: web site, website, internet site, site\n",
            "ep_f_strackzimmermann.jpg: theater curtain, theatre curtain\n",
            "ep_m_streit.jpg: suit, suit of clothes\n",
            "ep_f_steger.jpg: \n",
            "ep_f_strik.jpg: book jacket, dust cover, dust jacket, dust wrapper\n",
            "ep_f_strolenberg.jpg: \n",
            "ep_m_sypniewski.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_m_szczerba.jpg: web site, website, internet site, site\n",
            "ep_m_szydÅ‚o.jpg: \n",
            "ep_m_tarczynÌski.jpg: \n",
            "ep_f_tavares.jpg: \n",
            "ep_m_tegethoff.jpg: flagpole, flagstaff\n",
            "ep_f_teodorescumaÌŠwe.jpg: \n",
            "ep_m_terhesÌ§.jpg: web site, website, internet site, site\n",
            "ep_f_terlaak.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_terras.jpg: scoreboard\n",
            "ep_m_tertsch.jpg: \n",
            "ep_m_thionnet.jpg: \n",
            "ep_f_timgren.jpg: \n",
            "ep_f_tinagli.jpg: \n",
            "ep_m_tobeÌ.jpg: web site, website, internet site, site, analog clock\n",
            "ep_m_tomac.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_tomasÌŒicÌŒ.jpg: \n",
            "ep_f_tomc.jpg: flagpole, flagstaff\n",
            "ep_m_tonin.jpg: park bench\n",
            "ep_f_toom.jpg: \n",
            "ep_m_torselli.jpg: web site, website, internet site, site\n",
            "ep_m_tosi.jpg: web site, website, internet site, site\n",
            "ep_f_toussaint.jpg: \n",
            "ep_f_tovaglieri.JPG: envelope\n",
            "ep_m_toveri.jpg: web site, website, internet site, site\n",
            "ep_m_tynkkynen.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_uhriÌk.jpg: web site, website, internet site, site\n",
            "ep_f_vaidere.jpg: flagpole, flagstaff\n",
            "ep_f_vaÌ†lean.jpg: \n",
            "ep_m_valet.jpg: \n",
            "ep_m_vandenberg.jpg: lakeside, lakeshore\n",
            "ep_m_vandendriessche.jpg: comic book, web site, website, internet site, site\n",
            "ep_m_vanlanschot.jpg: academic gown, academic robe, judge's robe\n",
            "ep_f_vanleeuwen.jpg: promontory, headland, head, foreland, seashore, coast, seacoast, sea-coast\n",
            "ep_f_vautmans.jpg: pole\n",
            "ep_f_vedrenne.jpg: cinema, movie theater, movie theatre, movie house, picture palace\n",
            "ep_f_verheyen.jpg: \n",
            "ep_m_vesÌŒligaj.jpg: web site, website, internet site, site\n",
            "ep_f_vieira.jpg: \n",
            "ep_m_vilimsky.jpg: suit, suit of clothes, Windsor tie\n",
            "ep_m_vincze.jpg: swab, swob, mop\n",
            "ep_f_vind.jpg: web site, website, internet site, site\n",
            "ep_m_vistisen.jpg: web site, website, internet site, site, neck brace\n",
            "ep_m_volgin.jpg: \n",
            "ep_m_vondra.jpg: \n",
            "ep_m_voss.jpg: book jacket, dust cover, dust jacket, dust wrapper, web site, website, internet site, site\n",
            "ep_f_vozembergvrionidi.jpg: web site, website, internet site, site\n",
            "ep_f_vrecionovaÌ.jpg: hand-held computer, hand-held microcomputer\n",
            "ep_m_vaÌzquezlaÌzara.jpg: flagpole, flagstaff\n",
            "ep_m_waitz.jpg: digital clock, scoreboard\n",
            "ep_f_walsh.jpg: shower cap\n",
            "ep_f_walsmann.jpg: refrigerator, icebox\n",
            "ep_m_warborn.jpg: jersey, T-shirt, tee shirt\n",
            "ep_m_wawrykiewicz.jpg: suit, suit of clothes\n",
            "ep_f_wcisÅ‚o.jpg: \n",
            "ep_m_weber.jpg: \n",
            "ep_m_weimers.jpg: web site, website, internet site, site\n",
            "ep_f_wechsler.jpg: flagpole, flagstaff\n",
            "ep_f_wiesner.jpg: \n",
            "ep_m_wiezik.jpg: alp\n",
            "ep_f_wilmeÌ€s.jpg: television, television system\n",
            "ep_m_winkler.jpg: flagpole, flagstaff\n",
            "ep_f_winzig.jpg: web site, website, internet site, site\n",
            "ep_f_wiselerlima.jpg: suit, suit of clothes\n",
            "ep_f_wisÌniewska.jpg: torch\n",
            "ep_m_woÌˆlken.jpg: \n",
            "ep_f_wolters.jpg: planetarium\n",
            "ep_f_yar.jpg: academic gown, academic robe, judge's robe, mortarboard\n",
            "ep_f_yoncourtin.jpg: cinema, movie theater, movie theatre, movie house, picture palace\n",
            "ep_f_zajaÌ¨czkowskahernik.jpg: \n",
            "ep_f_zalewska.jpg: suit, suit of clothes\n",
            "ep_m_zÌŒalimas.jpg: flagpole, flagstaff\n",
            "ep_m_zan.jpg: academic gown, academic robe, judge's robe\n",
            "ep_m_zarzalejos.jpg: gondola\n",
            "ep_m_zdechovskyÌ.jpg: jersey, T-shirt, tee shirt\n",
            "ep_m_zdrojewski.jpg: \n",
            "ep_m_zÅ‚otowski.jpg: comic book\n",
            "ep_m_zoidoaÌlvarez.jpg: wardrobe, closet, press\n",
            "ep_f_zovko.jpg: suit, suit of clothes\n",
            "ep_m_zver.jpg: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown\n",
        "\n",
        "slide_content_chat3 = \"\"\"\n",
        "## Here we ask whether the computer can learn to classify male and female images\n",
        "\n",
        "- We pose the question the the Chat:\n",
        "    - Write Python code to run in Google Colab that goes through a folder of images in my Google Drive. The images are the banner pictures on social media of politicians. The folder in which my images sit is called ep_member_banner_img and is in the root or main directory. If an image name contains _f_ it is female and if _m_ it is male politician. I want you to try to learn to classify male and female images. You can suggest a library of package. I want you to look at 80 per cent of images, saving some for training/prediction. I want u to create a file with the results (describe the structure) and I want you to evaluate the accuracy\n",
        "\n",
        "- It gave the following code which:\n",
        "\t- It mounts Google Drive to access your image folder, imports standard libraries: torch, torchvision, PIL â†’ used for building and training the neural network, pandas and sklearn â†’ used for analyzing results and saving them neatly.\n",
        "\t- Defines the folder with images and where to save the output file (predicted_labels.tsv).\n",
        "\t- This is a small â€œhelperâ€ class (GenderDataset) that: finds all the .jpg, .jpeg, .png files in the folder, reads the label from the filename: _f_ means female â†’ label = 0, _m_ means male â†’ label = 1, anything else â†’ unknown (-1, ignored during training), loads each image in color (RGB), applies optional image transformations (like resizing and normalization), returns the image, its label, and the filename when requested. This class lets PyTorch handle images efficiently during training.\n",
        "\t- Transformations, Before feeding images to the model, theyâ€™re resized to 224Ã—224 pixels, converted to a PyTorch tensor, normalized (this just helps the model train better by scaling pixel values).\n",
        "\t- Creates the full dataset, keeps only images that actually have _f_ or _m_ in the filename, splits those into: 80% for training (to teach the model), 20% for testing (to check how well it learned), wraps them into â€œdata loadersâ€ that feed small batches (16 at a time) into the model â€” this makes training faster and less memory-intensive.\n",
        "\t- Loads a pre-trained ResNet-18 model â€” a popular CNN (Convolutional Neural Network) originally trained on millions of images, sets up: a loss function (CrossEntropy) to measure how wrong predictions are, an optimizer (Adam) to adjust the modelâ€™s weights, moves everything to GPU (cuda) if available, for speed, a third loader (all_loader) is used later to get predictions for all images.\n",
        "\t- Runs for 3 full passes (epochs) through the training data, for each batch of images: sends them to the model, gets predictions, calculates the loss (how far off the guesses were), adjusts the model to improve performance, after each epoch, it prints a simple â€œdoneâ€ message, this is the learning phase â€” the model gradually figures out what distinguishes male vs. female politiciansâ€™ images.\n",
        "\t- Turns off training mode and checks the model on the test set (the 20% of images it hasnâ€™t seen), collects predictions and compares them to the true labels, Calculates: Accuracy â†’ percentage of correct predictions, F1 score â†’ a balance between precision and recall, Prints a detailed classification report showing results for each class (female/male)\n",
        "\t- Now it uses the trained model to classify every image in the folder â€” even those without _f_ or _m_ in the name, for each image it records: Filename Modelâ€™s predicted label (0=female, 1=male), True label if known (or blank if not labeled), Whether that image was used during training (seen_in_training=1 or 0), Then it saves everything into a tab-separated file (predicted_labels.tsv) on Google Drive.\n",
        "\n",
        "- This takes a few minutes to run, it shows that male politicians can be predicted F1 score of 70 per cent but female not so much (generally F1 more than 70 is considered not bad). More generally, the message is that political communication *is* gendered - and invites us to figure out why or how\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(slide_content_chat3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "cellView": "form",
        "id": "aQ6MKNiT66FS",
        "outputId": "1318bcce-7537-471a-df7a-55cbe470e7d5"
      },
      "id": "aQ6MKNiT66FS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Here we ask whether the computer can learn to classify male and female images \n\n- We pose the question the the Chat:\n    - Write Python code to run in Google Colab that goes through a folder of images in my Google Drive. The images are the banner pictures on social media of politicians. The folder in which my images sit is called ep_member_banner_img and is in the root or main directory. If an image name contains _f_ it is female and if _m_ it is male politician. I want you to try to learn to classify male and female images. You can suggest a library of package. I want you to look at 80 per cent of images, saving some for training/prediction. I want u to create a file with the results (describe the structure) and I want you to evaluate the accuracy\n\n- It gave the following code which:\n\t- It mounts Google Drive to access your image folder, imports standard libraries: torch, torchvision, PIL â†’ used for building and training the neural network, pandas and sklearn â†’ used for analyzing results and saving them neatly.\n\t- Defines the folder with images and where to save the output file (predicted_labels.tsv).\n\t- This is a small â€œhelperâ€ class (GenderDataset) that: finds all the .jpg, .jpeg, .png files in the folder, reads the label from the filename: _f_ means female â†’ label = 0, _m_ means male â†’ label = 1, anything else â†’ unknown (-1, ignored during training), loads each image in color (RGB), applies optional image transformations (like resizing and normalization), returns the image, its label, and the filename when requested. This class lets PyTorch handle images efficiently during training.\n\t- Transformations, Before feeding images to the model, theyâ€™re resized to 224Ã—224 pixels, converted to a PyTorch tensor, normalized (this just helps the model train better by scaling pixel values).\n\t- Creates the full dataset, keeps only images that actually have _f_ or _m_ in the filename, splits those into: 80% for training (to teach the model), 20% for testing (to check how well it learned), wraps them into â€œdata loadersâ€ that feed small batches (16 at a time) into the model â€” this makes training faster and less memory-intensive.\n\t- Loads a pre-trained ResNet-18 model â€” a popular CNN (Convolutional Neural Network) originally trained on millions of images, sets up: a loss function (CrossEntropy) to measure how wrong predictions are, an optimizer (Adam) to adjust the modelâ€™s weights, moves everything to GPU (cuda) if available, for speed, a third loader (all_loader) is used later to get predictions for all images.\n\t- Runs for 3 full passes (epochs) through the training data, for each batch of images: sends them to the model, gets predictions, calculates the loss (how far off the guesses were), adjusts the model to improve performance, after each epoch, it prints a simple â€œdoneâ€ message, this is the learning phase â€” the model gradually figures out what distinguishes male vs. female politiciansâ€™ images.\n\t- Turns off training mode and checks the model on the test set (the 20% of images it hasnâ€™t seen), collects predictions and compares them to the true labels, Calculates: Accuracy â†’ percentage of correct predictions, F1 score â†’ a balance between precision and recall, Prints a detailed classification report showing results for each class (female/male)\n\t- Now it uses the trained model to classify every image in the folder â€” even those without _f_ or _m_ in the name, for each image it records: Filename Modelâ€™s predicted label (0=female, 1=male), True label if known (or blank if not labeled), Whether that image was used during training (seen_in_training=1 or 0), Then it saves everything into a tab-separated file (predicted_labels.tsv) on Google Drive.\n\n- This takes a few minutes to run, it shows that male politicians can be predicted F1 score of 70 per cent but female not so much (generally F1 more than 70 is considered not bad). More generally, the message is that political communication *is* gendered - and invites us to figure out why or how\n\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1ï¸âƒ£ Mount Google Drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# 2ï¸âƒ£ Imports\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# 3ï¸âƒ£ Paths\n",
        "folder_path = '/content/drive/MyDrive/ep_member_banner_img'\n",
        "output_path = '/content/drive/MyDrive/predicted_labels.tsv'  # TSV output\n",
        "\n",
        "# 4ï¸âƒ£ Custom Dataset\n",
        "class GenderDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "        self.labels = []\n",
        "        for f in self.files:\n",
        "            if \"_f_\" in f:\n",
        "                self.labels.append(0)  # female\n",
        "            elif \"_m_\" in f:\n",
        "                self.labels.append(1)  # male\n",
        "            else:\n",
        "                self.labels.append(-1)  # unknown / skip for training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.files[idx]\n",
        "        img_path = os.path.join(self.folder_path, filename)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, filename\n",
        "\n",
        "# 5ï¸âƒ£ Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 6ï¸âƒ£ Full dataset\n",
        "full_dataset = GenderDataset(folder_path, transform=transform)\n",
        "\n",
        "# 7ï¸âƒ£ Keep only labeled images for train/test split\n",
        "labeled_indices = [i for i, l in enumerate(full_dataset.labels) if l != -1]\n",
        "labeled_dataset = torch.utils.data.Subset(full_dataset, labeled_indices)\n",
        "\n",
        "# 8ï¸âƒ£ Train/test split\n",
        "train_size = int(0.8 * len(labeled_dataset))\n",
        "test_size = len(labeled_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(labeled_dataset, [train_size, test_size])\n",
        "\n",
        "# 9ï¸âƒ£ DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "all_loader = DataLoader(full_dataset, batch_size=16, shuffle=False)  # for predicting all\n",
        "\n",
        "# ðŸ”Ÿ Model setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)  # 2 classes: female/male\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 1ï¸âƒ£1ï¸âƒ£ Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels, _ in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} done\")\n",
        "\n",
        "# 1ï¸âƒ£2ï¸âƒ£ Evaluate on test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, _ in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Accuracy and F1\n",
        "accuracy = sum([p==l for p,l in zip(all_preds, all_labels)]) / len(all_labels)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test set overall accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"âœ… Test set overall F1 score: {f1:.4f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "report = classification_report(all_labels, all_preds, target_names=[\"female\", \"male\"])\n",
        "print(\"âœ… Per-class metrics:\\n\")\n",
        "print(report)\n",
        "\n",
        "# 1ï¸âƒ£3ï¸âƒ£ Predict on all images and save TSV with seen_in_training column\n",
        "# Get set of filenames used in training\n",
        "train_filenames = set([full_dataset.files[i] for i in train_dataset.indices])\n",
        "\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for images, labels, filenames in all_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        for fname, pred, true_label in zip(filenames, preds, labels.numpy()):\n",
        "            true_label_str = \"\" if true_label == -1 else str(true_label)\n",
        "            seen_flag = 1 if fname in train_filenames else 0\n",
        "            results.append((fname, pred, true_label_str, seen_flag))\n",
        "\n",
        "# Save TSV\n",
        "df = pd.DataFrame(results, columns=[\"filename\", \"predicted_label\", \"true_label\", \"seen_in_training\"])\n",
        "df.to_csv(output_path, sep='\\t', index=False)\n",
        "print(\"âœ… Predictions for all images saved to:\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQhQ8McCGok8",
        "outputId": "2054f845-3c13-4535-9b0e-a9b4d134667c"
      },
      "id": "sQhQ8McCGok8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 145MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 done\n",
            "Epoch 2/3 done\n",
            "Epoch 3/3 done\n",
            "\n",
            "âœ… Test set overall accuracy: 61.29%\n",
            "âœ… Test set overall F1 score: 0.7097\n",
            "âœ… Per-class metrics:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.48      0.37      0.42        35\n",
            "        male       0.67      0.76      0.71        58\n",
            "\n",
            "    accuracy                           0.61        93\n",
            "   macro avg       0.57      0.57      0.56        93\n",
            "weighted avg       0.60      0.61      0.60        93\n",
            "\n",
            "âœ… Predictions for all images saved to: /content/drive/MyDrive/predicted_labels.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yOzwnGF3e1kZ"
      },
      "id": "yOzwnGF3e1kZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0edcf64b1d3414d84915a665c4f98b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a17b9f4e084b42ada9476a4facd438f1",
              "IPY_MODEL_77bd4440f3824e969b34684ad97c5a84",
              "IPY_MODEL_5537e89b61004f1e97692d20bdef72d3"
            ],
            "layout": "IPY_MODEL_3dcd60fdbcd24c3da3b14e6cc0f3829f"
          }
        },
        "a17b9f4e084b42ada9476a4facd438f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6eb39f977044792a7b698aafe2c1419",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ad8c311af5074760bb56bb67911fe2bf",
            "value": "config.json:â€‡"
          }
        },
        "77bd4440f3824e969b34684ad97c5a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb423a2525314a1e9db4e6d24026f677",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_062a366822e146cf9a77d02d066705d8",
            "value": 1
          }
        },
        "5537e89b61004f1e97692d20bdef72d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfaa90899ef44a3db728447f4ab67a14",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7b759829486a49e6aac74a390a8f91f5",
            "value": "â€‡69.7k/?â€‡[00:00&lt;00:00,â€‡4.28MB/s]"
          }
        },
        "3dcd60fdbcd24c3da3b14e6cc0f3829f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6eb39f977044792a7b698aafe2c1419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad8c311af5074760bb56bb67911fe2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb423a2525314a1e9db4e6d24026f677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "062a366822e146cf9a77d02d066705d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfaa90899ef44a3db728447f4ab67a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b759829486a49e6aac74a390a8f91f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fda9499750f4dbe89b237524a4ee1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ccd26f9dab54a979be73357287e58cd",
              "IPY_MODEL_60b8a83cd84441ab826efa71de307384",
              "IPY_MODEL_fea1afae4f3746f3a785734fa7c4b9d6"
            ],
            "layout": "IPY_MODEL_a2121a2ee6b94e4495af315dd2e08c73"
          }
        },
        "1ccd26f9dab54a979be73357287e58cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99c38abfd9af48e0b9f95f9f75adbbb5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_95cffa6a168b446b99e3ff652b84052f",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "60b8a83cd84441ab826efa71de307384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bbe170f4a114f6c83ab122f58e9f28d",
            "max": 346293852,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97c8dc2f5dac4ab891243a2d0f5c7fe1",
            "value": 346293852
          }
        },
        "fea1afae4f3746f3a785734fa7c4b9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87f8c71866034023a2e2d6c5348462c5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b7d14bc651374a76b622705f02d23b6f",
            "value": "â€‡346M/346Mâ€‡[00:03&lt;00:00,â€‡155MB/s]"
          }
        },
        "a2121a2ee6b94e4495af315dd2e08c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c38abfd9af48e0b9f95f9f75adbbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95cffa6a168b446b99e3ff652b84052f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bbe170f4a114f6c83ab122f58e9f28d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97c8dc2f5dac4ab891243a2d0f5c7fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87f8c71866034023a2e2d6c5348462c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d14bc651374a76b622705f02d23b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78c148ebfc1b4a1eb3cbe21c36669d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edaa64f5d9c245d5b60958d603261afc",
              "IPY_MODEL_49c71f12d7434b80ba8d8fe0c2787e31",
              "IPY_MODEL_95af6ea5d426444b873f8ef1f26c9685"
            ],
            "layout": "IPY_MODEL_c49cbb0e418d4748ba18434f5b3f472a"
          }
        },
        "edaa64f5d9c245d5b60958d603261afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97ea37b1933b4e03ad5f96dabb40ab72",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4f79d9f71199463b907ec50c3f1c4343",
            "value": "preprocessor_config.json:â€‡100%"
          }
        },
        "49c71f12d7434b80ba8d8fe0c2787e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a475e276a5a341e98b5c5015939fbb10",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c462b49712ce4af19b32ac39f0aa71f7",
            "value": 160
          }
        },
        "95af6ea5d426444b873f8ef1f26c9685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2ed3f2d7be84a54a5fb29098c63053b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f45be24ed13d4c379639b73854ac2c53",
            "value": "â€‡160/160â€‡[00:00&lt;00:00,â€‡14.1kB/s]"
          }
        },
        "c49cbb0e418d4748ba18434f5b3f472a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97ea37b1933b4e03ad5f96dabb40ab72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f79d9f71199463b907ec50c3f1c4343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a475e276a5a341e98b5c5015939fbb10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c462b49712ce4af19b32ac39f0aa71f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2ed3f2d7be84a54a5fb29098c63053b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f45be24ed13d4c379639b73854ac2c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}